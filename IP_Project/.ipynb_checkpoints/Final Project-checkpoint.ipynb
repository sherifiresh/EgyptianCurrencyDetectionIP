{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.20971023\n",
      "Iteration 2, loss = 3.20909893\n",
      "Iteration 3, loss = 3.20823012\n",
      "Iteration 4, loss = 3.20713180\n",
      "Iteration 5, loss = 3.20582809\n",
      "Iteration 6, loss = 3.20433911\n",
      "Iteration 7, loss = 3.20268352\n",
      "Iteration 8, loss = 3.20088438\n",
      "Iteration 9, loss = 3.19895457\n",
      "Iteration 10, loss = 3.19691009\n",
      "Iteration 11, loss = 3.19476377\n",
      "Iteration 12, loss = 3.19253740\n",
      "Iteration 13, loss = 3.19024082\n",
      "Iteration 14, loss = 3.18788234\n",
      "Iteration 15, loss = 3.18546595\n",
      "Iteration 16, loss = 3.18299717\n",
      "Iteration 17, loss = 3.18050032\n",
      "Iteration 18, loss = 3.17797892\n",
      "Iteration 19, loss = 3.17544693\n",
      "Iteration 20, loss = 3.17289556\n",
      "Iteration 21, loss = 3.17032986\n",
      "Iteration 22, loss = 3.16775767\n",
      "Iteration 23, loss = 3.16518572\n",
      "Iteration 24, loss = 3.16260932\n",
      "Iteration 25, loss = 3.16002943\n",
      "Iteration 26, loss = 3.15745202\n",
      "Iteration 27, loss = 3.15487584\n",
      "Iteration 28, loss = 3.15229818\n",
      "Iteration 29, loss = 3.14971033\n",
      "Iteration 30, loss = 3.14713278\n",
      "Iteration 31, loss = 3.14458158\n",
      "Iteration 32, loss = 3.14205449\n",
      "Iteration 33, loss = 3.13954399\n",
      "Iteration 34, loss = 3.13704952\n",
      "Iteration 35, loss = 3.13456332\n",
      "Iteration 36, loss = 3.13208456\n",
      "Iteration 37, loss = 3.12961430\n",
      "Iteration 38, loss = 3.12716021\n",
      "Iteration 39, loss = 3.12471809\n",
      "Iteration 40, loss = 3.12229222\n",
      "Iteration 41, loss = 3.11988682\n",
      "Iteration 42, loss = 3.11749646\n",
      "Iteration 43, loss = 3.11511997\n",
      "Iteration 44, loss = 3.11276576\n",
      "Iteration 45, loss = 3.11042259\n",
      "Iteration 46, loss = 3.10808635\n",
      "Iteration 47, loss = 3.10576001\n",
      "Iteration 48, loss = 3.10344764\n",
      "Iteration 49, loss = 3.10114435\n",
      "Iteration 50, loss = 3.09885080\n",
      "Iteration 51, loss = 3.09656885\n",
      "Iteration 52, loss = 3.09429473\n",
      "Iteration 53, loss = 3.09204553\n",
      "Iteration 54, loss = 3.08981467\n",
      "Iteration 55, loss = 3.08759263\n",
      "Iteration 56, loss = 3.08537867\n",
      "Iteration 57, loss = 3.08317018\n",
      "Iteration 58, loss = 3.08097553\n",
      "Iteration 59, loss = 3.07879316\n",
      "Iteration 60, loss = 3.07661884\n",
      "Iteration 61, loss = 3.07444787\n",
      "Iteration 62, loss = 3.07228832\n",
      "Iteration 63, loss = 3.07014072\n",
      "Iteration 64, loss = 3.06800149\n",
      "Iteration 65, loss = 3.06587296\n",
      "Iteration 66, loss = 3.06374738\n",
      "Iteration 67, loss = 3.06163228\n",
      "Iteration 68, loss = 3.05952454\n",
      "Iteration 69, loss = 3.05742062\n",
      "Iteration 70, loss = 3.05532239\n",
      "Iteration 71, loss = 3.05323527\n",
      "Iteration 72, loss = 3.05116427\n",
      "Iteration 73, loss = 3.04910868\n",
      "Iteration 74, loss = 3.04706681\n",
      "Iteration 75, loss = 3.04503680\n",
      "Iteration 76, loss = 3.04302207\n",
      "Iteration 77, loss = 3.04101411\n",
      "Iteration 78, loss = 3.03901555\n",
      "Iteration 79, loss = 3.03702569\n",
      "Iteration 80, loss = 3.03504243\n",
      "Iteration 81, loss = 3.03306513\n",
      "Iteration 82, loss = 3.03109793\n",
      "Iteration 83, loss = 3.02913583\n",
      "Iteration 84, loss = 3.02717636\n",
      "Iteration 85, loss = 3.02521752\n",
      "Iteration 86, loss = 3.02326424\n",
      "Iteration 87, loss = 3.02131532\n",
      "Iteration 88, loss = 3.01937088\n",
      "Iteration 89, loss = 3.01742867\n",
      "Iteration 90, loss = 3.01548921\n",
      "Iteration 91, loss = 3.01355366\n",
      "Iteration 92, loss = 3.01162226\n",
      "Iteration 93, loss = 3.00969908\n",
      "Iteration 94, loss = 3.00778367\n",
      "Iteration 95, loss = 3.00587440\n",
      "Iteration 96, loss = 3.00396913\n",
      "Iteration 97, loss = 3.00206962\n",
      "Iteration 98, loss = 3.00017264\n",
      "Iteration 99, loss = 2.99827641\n",
      "Iteration 100, loss = 2.99637855\n",
      "Iteration 101, loss = 2.99448429\n",
      "Iteration 102, loss = 2.99259506\n",
      "Iteration 103, loss = 2.99071422\n",
      "Iteration 104, loss = 2.98883609\n",
      "Iteration 105, loss = 2.98696154\n",
      "Iteration 106, loss = 2.98509142\n",
      "Iteration 107, loss = 2.98322783\n",
      "Iteration 108, loss = 2.98136200\n",
      "Iteration 109, loss = 2.97950123\n",
      "Iteration 110, loss = 2.97764673\n",
      "Iteration 111, loss = 2.97579538\n",
      "Iteration 112, loss = 2.97395160\n",
      "Iteration 113, loss = 2.97211700\n",
      "Iteration 114, loss = 2.97028570\n",
      "Iteration 115, loss = 2.96845478\n",
      "Iteration 116, loss = 2.96662411\n",
      "Iteration 117, loss = 2.96479763\n",
      "Iteration 118, loss = 2.96297592\n",
      "Iteration 119, loss = 2.96115651\n",
      "Iteration 120, loss = 2.95933934\n",
      "Iteration 121, loss = 2.95752096\n",
      "Iteration 122, loss = 2.95569919\n",
      "Iteration 123, loss = 2.95387811\n",
      "Iteration 124, loss = 2.95205928\n",
      "Iteration 125, loss = 2.95024075\n",
      "Iteration 126, loss = 2.94842291\n",
      "Iteration 127, loss = 2.94660756\n",
      "Iteration 128, loss = 2.94479418\n",
      "Iteration 129, loss = 2.94298279\n",
      "Iteration 130, loss = 2.94117033\n",
      "Iteration 131, loss = 2.93935939\n",
      "Iteration 132, loss = 2.93754931\n",
      "Iteration 133, loss = 2.93574098\n",
      "Iteration 134, loss = 2.93393575\n",
      "Iteration 135, loss = 2.93213437\n",
      "Iteration 136, loss = 2.93033496\n",
      "Iteration 137, loss = 2.92853553\n",
      "Iteration 138, loss = 2.92673293\n",
      "Iteration 139, loss = 2.92492864\n",
      "Iteration 140, loss = 2.92312368\n",
      "Iteration 141, loss = 2.92131698\n",
      "Iteration 142, loss = 2.91950876\n",
      "Iteration 143, loss = 2.91769898\n",
      "Iteration 144, loss = 2.91589013\n",
      "Iteration 145, loss = 2.91408168\n",
      "Iteration 146, loss = 2.91227212\n",
      "Iteration 147, loss = 2.91046268\n",
      "Iteration 148, loss = 2.90865579\n",
      "Iteration 149, loss = 2.90684981\n",
      "Iteration 150, loss = 2.90504267\n",
      "Iteration 151, loss = 2.90323194\n",
      "Iteration 152, loss = 2.90142149\n",
      "Iteration 153, loss = 2.89961370\n",
      "Iteration 154, loss = 2.89780310\n",
      "Iteration 155, loss = 2.89599289\n",
      "Iteration 156, loss = 2.89417890\n",
      "Iteration 157, loss = 2.89236511\n",
      "Iteration 158, loss = 2.89054950\n",
      "Iteration 159, loss = 2.88873131\n",
      "Iteration 160, loss = 2.88691090\n",
      "Iteration 161, loss = 2.88509047\n",
      "Iteration 162, loss = 2.88327030\n",
      "Iteration 163, loss = 2.88144810\n",
      "Iteration 164, loss = 2.87962397\n",
      "Iteration 165, loss = 2.87779782\n",
      "Iteration 166, loss = 2.87597102\n",
      "Iteration 167, loss = 2.87414461\n",
      "Iteration 168, loss = 2.87231863\n",
      "Iteration 169, loss = 2.87049275\n",
      "Iteration 170, loss = 2.86866730\n",
      "Iteration 171, loss = 2.86684008\n",
      "Iteration 172, loss = 2.86501149\n",
      "Iteration 173, loss = 2.86318464\n",
      "Iteration 174, loss = 2.86135798\n",
      "Iteration 175, loss = 2.85953186\n",
      "Iteration 176, loss = 2.85770356\n",
      "Iteration 177, loss = 2.85587490\n",
      "Iteration 178, loss = 2.85404753\n",
      "Iteration 179, loss = 2.85222135\n",
      "Iteration 180, loss = 2.85039472\n",
      "Iteration 181, loss = 2.84856831\n",
      "Iteration 182, loss = 2.84673901\n",
      "Iteration 183, loss = 2.84490754\n",
      "Iteration 184, loss = 2.84307506\n",
      "Iteration 185, loss = 2.84124346\n",
      "Iteration 186, loss = 2.83941236\n",
      "Iteration 187, loss = 2.83758033\n",
      "Iteration 188, loss = 2.83574725\n",
      "Iteration 189, loss = 2.83391493\n",
      "Iteration 190, loss = 2.83208226\n",
      "Iteration 191, loss = 2.83024999\n",
      "Iteration 192, loss = 2.82841883\n",
      "Iteration 193, loss = 2.82658665\n",
      "Iteration 194, loss = 2.82475269\n",
      "Iteration 195, loss = 2.82291780\n",
      "Iteration 196, loss = 2.82108197\n",
      "Iteration 197, loss = 2.81924548\n",
      "Iteration 198, loss = 2.81740827\n",
      "Iteration 199, loss = 2.81557033\n",
      "Iteration 200, loss = 2.81373174\n",
      "Iteration 201, loss = 2.81189386\n",
      "Iteration 202, loss = 2.81005485\n",
      "Iteration 203, loss = 2.80821355\n",
      "Iteration 204, loss = 2.80636996\n",
      "Iteration 205, loss = 2.80452505\n",
      "Iteration 206, loss = 2.80268018\n",
      "Iteration 207, loss = 2.80083370\n",
      "Iteration 208, loss = 2.79898617\n",
      "Iteration 209, loss = 2.79713730\n",
      "Iteration 210, loss = 2.79528683\n",
      "Iteration 211, loss = 2.79343412\n",
      "Iteration 212, loss = 2.79158135\n",
      "Iteration 213, loss = 2.78972525\n",
      "Iteration 214, loss = 2.78786718\n",
      "Iteration 215, loss = 2.78600742\n",
      "Iteration 216, loss = 2.78414631\n",
      "Iteration 217, loss = 2.78228176\n",
      "Iteration 218, loss = 2.78041427\n",
      "Iteration 219, loss = 2.77854569\n",
      "Iteration 220, loss = 2.77667701\n",
      "Iteration 221, loss = 2.77480686\n",
      "Iteration 222, loss = 2.77293410\n",
      "Iteration 223, loss = 2.77106241\n",
      "Iteration 224, loss = 2.76918940\n",
      "Iteration 225, loss = 2.76731620\n",
      "Iteration 226, loss = 2.76544164\n",
      "Iteration 227, loss = 2.76356633\n",
      "Iteration 228, loss = 2.76169100\n",
      "Iteration 229, loss = 2.75981511\n",
      "Iteration 230, loss = 2.75793893\n",
      "Iteration 231, loss = 2.75606174\n",
      "Iteration 232, loss = 2.75418401\n",
      "Iteration 233, loss = 2.75230443\n",
      "Iteration 234, loss = 2.75042432\n",
      "Iteration 235, loss = 2.74854181\n",
      "Iteration 236, loss = 2.74665860\n",
      "Iteration 237, loss = 2.74477316\n",
      "Iteration 238, loss = 2.74288663\n",
      "Iteration 239, loss = 2.74099665\n",
      "Iteration 240, loss = 2.73910620\n",
      "Iteration 241, loss = 2.73721601\n",
      "Iteration 242, loss = 2.73532514\n",
      "Iteration 243, loss = 2.73343371\n",
      "Iteration 244, loss = 2.73154134\n",
      "Iteration 245, loss = 2.72964665\n",
      "Iteration 246, loss = 2.72774881\n",
      "Iteration 247, loss = 2.72585059\n",
      "Iteration 248, loss = 2.72395214\n",
      "Iteration 249, loss = 2.72205382\n",
      "Iteration 250, loss = 2.72015486\n",
      "Iteration 251, loss = 2.71825495\n",
      "Iteration 252, loss = 2.71635299\n",
      "Iteration 253, loss = 2.71444985\n",
      "Iteration 254, loss = 2.71254705\n",
      "Iteration 255, loss = 2.71064386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 256, loss = 2.70874016\n",
      "Iteration 257, loss = 2.70683373\n",
      "Iteration 258, loss = 2.70492733\n",
      "Iteration 259, loss = 2.70302062\n",
      "Iteration 260, loss = 2.70111280\n",
      "Iteration 261, loss = 2.69920200\n",
      "Iteration 262, loss = 2.69728979\n",
      "Iteration 263, loss = 2.69537588\n",
      "Iteration 264, loss = 2.69345900\n",
      "Iteration 265, loss = 2.69153923\n",
      "Iteration 266, loss = 2.68961849\n",
      "Iteration 267, loss = 2.68769430\n",
      "Iteration 268, loss = 2.68576798\n",
      "Iteration 269, loss = 2.68384026\n",
      "Iteration 270, loss = 2.68191103\n",
      "Iteration 271, loss = 2.67997910\n",
      "Iteration 272, loss = 2.67804560\n",
      "Iteration 273, loss = 2.67611088\n",
      "Iteration 274, loss = 2.67417381\n",
      "Iteration 275, loss = 2.67223423\n",
      "Iteration 276, loss = 2.67029147\n",
      "Iteration 277, loss = 2.66834894\n",
      "Iteration 278, loss = 2.66640515\n",
      "Iteration 279, loss = 2.66445955\n",
      "Iteration 280, loss = 2.66250997\n",
      "Iteration 281, loss = 2.66055890\n",
      "Iteration 282, loss = 2.65860485\n",
      "Iteration 283, loss = 2.65664769\n",
      "Iteration 284, loss = 2.65468721\n",
      "Iteration 285, loss = 2.65272306\n",
      "Iteration 286, loss = 2.65075530\n",
      "Iteration 287, loss = 2.64878582\n",
      "Iteration 288, loss = 2.64681561\n",
      "Iteration 289, loss = 2.64484274\n",
      "Iteration 290, loss = 2.64286702\n",
      "Iteration 291, loss = 2.64088825\n",
      "Iteration 292, loss = 2.63890896\n",
      "Iteration 293, loss = 2.63692842\n",
      "Iteration 294, loss = 2.63494569\n",
      "Iteration 295, loss = 2.63296128\n",
      "Iteration 296, loss = 2.63097604\n",
      "Iteration 297, loss = 2.62898942\n",
      "Iteration 298, loss = 2.62700238\n",
      "Iteration 299, loss = 2.62501272\n",
      "Iteration 300, loss = 2.62302255\n",
      "Iteration 301, loss = 2.62103168\n",
      "Iteration 302, loss = 2.61903970\n",
      "Iteration 303, loss = 2.61704335\n",
      "Iteration 304, loss = 2.61504329\n",
      "Iteration 305, loss = 2.61304175\n",
      "Iteration 306, loss = 2.61103823\n",
      "Iteration 307, loss = 2.60903087\n",
      "Iteration 308, loss = 2.60702265\n",
      "Iteration 309, loss = 2.60501082\n",
      "Iteration 310, loss = 2.60299745\n",
      "Iteration 311, loss = 2.60098404\n",
      "Iteration 312, loss = 2.59896785\n",
      "Iteration 313, loss = 2.59694866\n",
      "Iteration 314, loss = 2.59492625\n",
      "Iteration 315, loss = 2.59290173\n",
      "Iteration 316, loss = 2.59087482\n",
      "Iteration 317, loss = 2.58884285\n",
      "Iteration 318, loss = 2.58680732\n",
      "Iteration 319, loss = 2.58476949\n",
      "Iteration 320, loss = 2.58273175\n",
      "Iteration 321, loss = 2.58069277\n",
      "Iteration 322, loss = 2.57865390\n",
      "Iteration 323, loss = 2.57661530\n",
      "Iteration 324, loss = 2.57457552\n",
      "Iteration 325, loss = 2.57253179\n",
      "Iteration 326, loss = 2.57048446\n",
      "Iteration 327, loss = 2.56843786\n",
      "Iteration 328, loss = 2.56639051\n",
      "Iteration 329, loss = 2.56434428\n",
      "Iteration 330, loss = 2.56229749\n",
      "Iteration 331, loss = 2.56025011\n",
      "Iteration 332, loss = 2.55820134\n",
      "Iteration 333, loss = 2.55615343\n",
      "Iteration 334, loss = 2.55410437\n",
      "Iteration 335, loss = 2.55205218\n",
      "Iteration 336, loss = 2.54999782\n",
      "Iteration 337, loss = 2.54794113\n",
      "Iteration 338, loss = 2.54588400\n",
      "Iteration 339, loss = 2.54382402\n",
      "Iteration 340, loss = 2.54176169\n",
      "Iteration 341, loss = 2.53969802\n",
      "Iteration 342, loss = 2.53763215\n",
      "Iteration 343, loss = 2.53556298\n",
      "Iteration 344, loss = 2.53349047\n",
      "Iteration 345, loss = 2.53141372\n",
      "Iteration 346, loss = 2.52933532\n",
      "Iteration 347, loss = 2.52725732\n",
      "Iteration 348, loss = 2.52518001\n",
      "Iteration 349, loss = 2.52310342\n",
      "Iteration 350, loss = 2.52102319\n",
      "Iteration 351, loss = 2.51893842\n",
      "Iteration 352, loss = 2.51685086\n",
      "Iteration 353, loss = 2.51475918\n",
      "Iteration 354, loss = 2.51266671\n",
      "Iteration 355, loss = 2.51057467\n",
      "Iteration 356, loss = 2.50848231\n",
      "Iteration 357, loss = 2.50639006\n",
      "Iteration 358, loss = 2.50429900\n",
      "Iteration 359, loss = 2.50220637\n",
      "Iteration 360, loss = 2.50011477\n",
      "Iteration 361, loss = 2.49802114\n",
      "Iteration 362, loss = 2.49592785\n",
      "Iteration 363, loss = 2.49383446\n",
      "Iteration 364, loss = 2.49173992\n",
      "Iteration 365, loss = 2.48964533\n",
      "Iteration 366, loss = 2.48755034\n",
      "Iteration 367, loss = 2.48545546\n",
      "Iteration 368, loss = 2.48335812\n",
      "Iteration 369, loss = 2.48125858\n",
      "Iteration 370, loss = 2.47915846\n",
      "Iteration 371, loss = 2.47705653\n",
      "Iteration 372, loss = 2.47495419\n",
      "Iteration 373, loss = 2.47285123\n",
      "Iteration 374, loss = 2.47074712\n",
      "Iteration 375, loss = 2.46864083\n",
      "Iteration 376, loss = 2.46653404\n",
      "Iteration 377, loss = 2.46442691\n",
      "Iteration 378, loss = 2.46231762\n",
      "Iteration 379, loss = 2.46020773\n",
      "Iteration 380, loss = 2.45809558\n",
      "Iteration 381, loss = 2.45598265\n",
      "Iteration 382, loss = 2.45386760\n",
      "Iteration 383, loss = 2.45175092\n",
      "Iteration 384, loss = 2.44963394\n",
      "Iteration 385, loss = 2.44751679\n",
      "Iteration 386, loss = 2.44539932\n",
      "Iteration 387, loss = 2.44328160\n",
      "Iteration 388, loss = 2.44116406\n",
      "Iteration 389, loss = 2.43904621\n",
      "Iteration 390, loss = 2.43692776\n",
      "Iteration 391, loss = 2.43480766\n",
      "Iteration 392, loss = 2.43268646\n",
      "Iteration 393, loss = 2.43056561\n",
      "Iteration 394, loss = 2.42844455\n",
      "Iteration 395, loss = 2.42632294\n",
      "Iteration 396, loss = 2.42419953\n",
      "Iteration 397, loss = 2.42207448\n",
      "Iteration 398, loss = 2.41994942\n",
      "Iteration 399, loss = 2.41782399\n",
      "Iteration 400, loss = 2.41569631\n",
      "Iteration 401, loss = 2.41356717\n",
      "Iteration 402, loss = 2.41143767\n",
      "Iteration 403, loss = 2.40930735\n",
      "Iteration 404, loss = 2.40717592\n",
      "Iteration 405, loss = 2.40504255\n",
      "Iteration 406, loss = 2.40290786\n",
      "Iteration 407, loss = 2.40077179\n",
      "Iteration 408, loss = 2.39863485\n",
      "Iteration 409, loss = 2.39649579\n",
      "Iteration 410, loss = 2.39435593\n",
      "Iteration 411, loss = 2.39221599\n",
      "Iteration 412, loss = 2.39007376\n",
      "Iteration 413, loss = 2.38792939\n",
      "Iteration 414, loss = 2.38578415\n",
      "Iteration 415, loss = 2.38363937\n",
      "Iteration 416, loss = 2.38149477\n",
      "Iteration 417, loss = 2.37934831\n",
      "Iteration 418, loss = 2.37720004\n",
      "Iteration 419, loss = 2.37505154\n",
      "Iteration 420, loss = 2.37290209\n",
      "Iteration 421, loss = 2.37075277\n",
      "Iteration 422, loss = 2.36860187\n",
      "Iteration 423, loss = 2.36645016\n",
      "Iteration 424, loss = 2.36429674\n",
      "Iteration 425, loss = 2.36214173\n",
      "Iteration 426, loss = 2.35998540\n",
      "Iteration 427, loss = 2.35782614\n",
      "Iteration 428, loss = 2.35566514\n",
      "Iteration 429, loss = 2.35350372\n",
      "Iteration 430, loss = 2.35134282\n",
      "Iteration 431, loss = 2.34918143\n",
      "Iteration 432, loss = 2.34701944\n",
      "Iteration 433, loss = 2.34485563\n",
      "Iteration 434, loss = 2.34269027\n",
      "Iteration 435, loss = 2.34052393\n",
      "Iteration 436, loss = 2.33835702\n",
      "Iteration 437, loss = 2.33618877\n",
      "Iteration 438, loss = 2.33401825\n",
      "Iteration 439, loss = 2.33184634\n",
      "Iteration 440, loss = 2.32967322\n",
      "Iteration 441, loss = 2.32749844\n",
      "Iteration 442, loss = 2.32532304\n",
      "Iteration 443, loss = 2.32314697\n",
      "Iteration 444, loss = 2.32096940\n",
      "Iteration 445, loss = 2.31879006\n",
      "Iteration 446, loss = 2.31660910\n",
      "Iteration 447, loss = 2.31442539\n",
      "Iteration 448, loss = 2.31224127\n",
      "Iteration 449, loss = 2.31005631\n",
      "Iteration 450, loss = 2.30787014\n",
      "Iteration 451, loss = 2.30568344\n",
      "Iteration 452, loss = 2.30349530\n",
      "Iteration 453, loss = 2.30130517\n",
      "Iteration 454, loss = 2.29911420\n",
      "Iteration 455, loss = 2.29692284\n",
      "Iteration 456, loss = 2.29473062\n",
      "Iteration 457, loss = 2.29253759\n",
      "Iteration 458, loss = 2.29034319\n",
      "Iteration 459, loss = 2.28814772\n",
      "Iteration 460, loss = 2.28594956\n",
      "Iteration 461, loss = 2.28375080\n",
      "Iteration 462, loss = 2.28155126\n",
      "Iteration 463, loss = 2.27935141\n",
      "Iteration 464, loss = 2.27715058\n",
      "Iteration 465, loss = 2.27494872\n",
      "Iteration 466, loss = 2.27274571\n",
      "Iteration 467, loss = 2.27054116\n",
      "Iteration 468, loss = 2.26833554\n",
      "Iteration 469, loss = 2.26612670\n",
      "Iteration 470, loss = 2.26391635\n",
      "Iteration 471, loss = 2.26170353\n",
      "Iteration 472, loss = 2.25948933\n",
      "Iteration 473, loss = 2.25727497\n",
      "Iteration 474, loss = 2.25505887\n",
      "Iteration 475, loss = 2.25284160\n",
      "Iteration 476, loss = 2.25062361\n",
      "Iteration 477, loss = 2.24840295\n",
      "Iteration 478, loss = 2.24617882\n",
      "Iteration 479, loss = 2.24395255\n",
      "Iteration 480, loss = 2.24172531\n",
      "Iteration 481, loss = 2.23949579\n",
      "Iteration 482, loss = 2.23726789\n",
      "Iteration 483, loss = 2.23504030\n",
      "Iteration 484, loss = 2.23281037\n",
      "Iteration 485, loss = 2.23057909\n",
      "Iteration 486, loss = 2.22834720\n",
      "Iteration 487, loss = 2.22611471\n",
      "Iteration 488, loss = 2.22388167\n",
      "Iteration 489, loss = 2.22164645\n",
      "Iteration 490, loss = 2.21941025\n",
      "Iteration 491, loss = 2.21717475\n",
      "Iteration 492, loss = 2.21493800\n",
      "Iteration 493, loss = 2.21269987\n",
      "Iteration 494, loss = 2.21046051\n",
      "Iteration 495, loss = 2.20822170\n",
      "Iteration 496, loss = 2.20598356\n",
      "Iteration 497, loss = 2.20374372\n",
      "Iteration 498, loss = 2.20150407\n",
      "Iteration 499, loss = 2.19926325\n",
      "Iteration 500, loss = 2.19702046\n",
      "Iteration 501, loss = 2.19477598\n",
      "Iteration 502, loss = 2.19252879\n",
      "Iteration 503, loss = 2.19028131\n",
      "Iteration 504, loss = 2.18803295\n",
      "Iteration 505, loss = 2.18578382\n",
      "Iteration 506, loss = 2.18353346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 507, loss = 2.18128282\n",
      "Iteration 508, loss = 2.17903171\n",
      "Iteration 509, loss = 2.17677980\n",
      "Iteration 510, loss = 2.17452608\n",
      "Iteration 511, loss = 2.17227014\n",
      "Iteration 512, loss = 2.17001411\n",
      "Iteration 513, loss = 2.16775688\n",
      "Iteration 514, loss = 2.16550031\n",
      "Iteration 515, loss = 2.16324309\n",
      "Iteration 516, loss = 2.16098450\n",
      "Iteration 517, loss = 2.15872478\n",
      "Iteration 518, loss = 2.15646325\n",
      "Iteration 519, loss = 2.15420149\n",
      "Iteration 520, loss = 2.15193885\n",
      "Iteration 521, loss = 2.14967446\n",
      "Iteration 522, loss = 2.14740954\n",
      "Iteration 523, loss = 2.14514280\n",
      "Iteration 524, loss = 2.14287417\n",
      "Iteration 525, loss = 2.14060472\n",
      "Iteration 526, loss = 2.13833306\n",
      "Iteration 527, loss = 2.13606062\n",
      "Iteration 528, loss = 2.13378606\n",
      "Iteration 529, loss = 2.13150824\n",
      "Iteration 530, loss = 2.12922869\n",
      "Iteration 531, loss = 2.12694832\n",
      "Iteration 532, loss = 2.12466734\n",
      "Iteration 533, loss = 2.12238488\n",
      "Iteration 534, loss = 2.12010209\n",
      "Iteration 535, loss = 2.11781684\n",
      "Iteration 536, loss = 2.11553221\n",
      "Iteration 537, loss = 2.11324863\n",
      "Iteration 538, loss = 2.11096450\n",
      "Iteration 539, loss = 2.10867944\n",
      "Iteration 540, loss = 2.10639392\n",
      "Iteration 541, loss = 2.10410754\n",
      "Iteration 542, loss = 2.10182340\n",
      "Iteration 543, loss = 2.09953870\n",
      "Iteration 544, loss = 2.09725292\n",
      "Iteration 545, loss = 2.09496683\n",
      "Iteration 546, loss = 2.09268092\n",
      "Iteration 547, loss = 2.09039499\n",
      "Iteration 548, loss = 2.08810881\n",
      "Iteration 549, loss = 2.08582259\n",
      "Iteration 550, loss = 2.08353412\n",
      "Iteration 551, loss = 2.08124420\n",
      "Iteration 552, loss = 2.07895104\n",
      "Iteration 553, loss = 2.07665691\n",
      "Iteration 554, loss = 2.07436298\n",
      "Iteration 555, loss = 2.07206854\n",
      "Iteration 556, loss = 2.06977479\n",
      "Iteration 557, loss = 2.06747927\n",
      "Iteration 558, loss = 2.06518185\n",
      "Iteration 559, loss = 2.06288361\n",
      "Iteration 560, loss = 2.06058562\n",
      "Iteration 561, loss = 2.05828703\n",
      "Iteration 562, loss = 2.05598822\n",
      "Iteration 563, loss = 2.05368840\n",
      "Iteration 564, loss = 2.05138777\n",
      "Iteration 565, loss = 2.04908692\n",
      "Iteration 566, loss = 2.04678542\n",
      "Iteration 567, loss = 2.04448246\n",
      "Iteration 568, loss = 2.04217624\n",
      "Iteration 569, loss = 2.03986883\n",
      "Iteration 570, loss = 2.03756189\n",
      "Iteration 571, loss = 2.03525568\n",
      "Iteration 572, loss = 2.03295310\n",
      "Iteration 573, loss = 2.03065187\n",
      "Iteration 574, loss = 2.02835290\n",
      "Iteration 575, loss = 2.02605480\n",
      "Iteration 576, loss = 2.02376033\n",
      "Iteration 577, loss = 2.02146562\n",
      "Iteration 578, loss = 2.01917222\n",
      "Iteration 579, loss = 2.01688020\n",
      "Iteration 580, loss = 2.01458885\n",
      "Iteration 581, loss = 2.01229735\n",
      "Iteration 582, loss = 2.01000625\n",
      "Iteration 583, loss = 2.00771535\n",
      "Iteration 584, loss = 2.00542466\n",
      "Iteration 585, loss = 2.00313325\n",
      "Iteration 586, loss = 2.00084004\n",
      "Iteration 587, loss = 1.99854665\n",
      "Iteration 588, loss = 1.99625149\n",
      "Iteration 589, loss = 1.99395661\n",
      "Iteration 590, loss = 1.99166045\n",
      "Iteration 591, loss = 1.98936482\n",
      "Iteration 592, loss = 1.98706984\n",
      "Iteration 593, loss = 1.98477483\n",
      "Iteration 594, loss = 1.98247902\n",
      "Iteration 595, loss = 1.98018392\n",
      "Iteration 596, loss = 1.97788720\n",
      "Iteration 597, loss = 1.97558965\n",
      "Iteration 598, loss = 1.97328969\n",
      "Iteration 599, loss = 1.97099010\n",
      "Iteration 600, loss = 1.96868907\n",
      "Iteration 601, loss = 1.96638665\n",
      "Iteration 602, loss = 1.96408268\n",
      "Iteration 603, loss = 1.96177893\n",
      "Iteration 604, loss = 1.95947536\n",
      "Iteration 605, loss = 1.95717239\n",
      "Iteration 606, loss = 1.95487051\n",
      "Iteration 607, loss = 1.95256793\n",
      "Iteration 608, loss = 1.95026397\n",
      "Iteration 609, loss = 1.94796061\n",
      "Iteration 610, loss = 1.94565735\n",
      "Iteration 611, loss = 1.94335421\n",
      "Iteration 612, loss = 1.94105136\n",
      "Iteration 613, loss = 1.93874713\n",
      "Iteration 614, loss = 1.93644186\n",
      "Iteration 615, loss = 1.93413681\n",
      "Iteration 616, loss = 1.93183195\n",
      "Iteration 617, loss = 1.92952703\n",
      "Iteration 618, loss = 1.92722332\n",
      "Iteration 619, loss = 1.92492073\n",
      "Iteration 620, loss = 1.92261928\n",
      "Iteration 621, loss = 1.92031736\n",
      "Iteration 622, loss = 1.91801812\n",
      "Iteration 623, loss = 1.91571916\n",
      "Iteration 624, loss = 1.91342073\n",
      "Iteration 625, loss = 1.91112276\n",
      "Iteration 626, loss = 1.90882521\n",
      "Iteration 627, loss = 1.90652768\n",
      "Iteration 628, loss = 1.90423037\n",
      "Iteration 629, loss = 1.90193479\n",
      "Iteration 630, loss = 1.89963802\n",
      "Iteration 631, loss = 1.89734162\n",
      "Iteration 632, loss = 1.89504517\n",
      "Iteration 633, loss = 1.89274929\n",
      "Iteration 634, loss = 1.89045250\n",
      "Iteration 635, loss = 1.88815564\n",
      "Iteration 636, loss = 1.88585978\n",
      "Iteration 637, loss = 1.88356392\n",
      "Iteration 638, loss = 1.88126810\n",
      "Iteration 639, loss = 1.87897256\n",
      "Iteration 640, loss = 1.87667597\n",
      "Iteration 641, loss = 1.87437950\n",
      "Iteration 642, loss = 1.87208389\n",
      "Iteration 643, loss = 1.86978891\n",
      "Iteration 644, loss = 1.86749443\n",
      "Iteration 645, loss = 1.86520033\n",
      "Iteration 646, loss = 1.86290552\n",
      "Iteration 647, loss = 1.86061067\n",
      "Iteration 648, loss = 1.85831526\n",
      "Iteration 649, loss = 1.85602112\n",
      "Iteration 650, loss = 1.85372714\n",
      "Iteration 651, loss = 1.85143275\n",
      "Iteration 652, loss = 1.84913637\n",
      "Iteration 653, loss = 1.84683985\n",
      "Iteration 654, loss = 1.84454364\n",
      "Iteration 655, loss = 1.84224686\n",
      "Iteration 656, loss = 1.83994985\n",
      "Iteration 657, loss = 1.83765232\n",
      "Iteration 658, loss = 1.83535451\n",
      "Iteration 659, loss = 1.83305707\n",
      "Iteration 660, loss = 1.83075971\n",
      "Iteration 661, loss = 1.82846199\n",
      "Iteration 662, loss = 1.82616361\n",
      "Iteration 663, loss = 1.82386583\n",
      "Iteration 664, loss = 1.82156802\n",
      "Iteration 665, loss = 1.81926954\n",
      "Iteration 666, loss = 1.81697122\n",
      "Iteration 667, loss = 1.81467268\n",
      "Iteration 668, loss = 1.81237449\n",
      "Iteration 669, loss = 1.81007695\n",
      "Iteration 670, loss = 1.80778083\n",
      "Iteration 671, loss = 1.80548521\n",
      "Iteration 672, loss = 1.80319070\n",
      "Iteration 673, loss = 1.80089667\n",
      "Iteration 674, loss = 1.79860204\n",
      "Iteration 675, loss = 1.79630819\n",
      "Iteration 676, loss = 1.79401552\n",
      "Iteration 677, loss = 1.79172273\n",
      "Iteration 678, loss = 1.78943059\n",
      "Iteration 679, loss = 1.78713789\n",
      "Iteration 680, loss = 1.78484626\n",
      "Iteration 681, loss = 1.78255633\n",
      "Iteration 682, loss = 1.78026804\n",
      "Iteration 683, loss = 1.77798018\n",
      "Iteration 684, loss = 1.77569322\n",
      "Iteration 685, loss = 1.77340749\n",
      "Iteration 686, loss = 1.77112042\n",
      "Iteration 687, loss = 1.76883386\n",
      "Iteration 688, loss = 1.76654908\n",
      "Iteration 689, loss = 1.76426552\n",
      "Iteration 690, loss = 1.76198348\n",
      "Iteration 691, loss = 1.75970115\n",
      "Iteration 692, loss = 1.75741856\n",
      "Iteration 693, loss = 1.75513568\n",
      "Iteration 694, loss = 1.75285321\n",
      "Iteration 695, loss = 1.75057150\n",
      "Iteration 696, loss = 1.74828996\n",
      "Iteration 697, loss = 1.74600858\n",
      "Iteration 698, loss = 1.74372673\n",
      "Iteration 699, loss = 1.74144599\n",
      "Iteration 700, loss = 1.73916635\n",
      "Iteration 701, loss = 1.73688804\n",
      "Iteration 702, loss = 1.73460900\n",
      "Iteration 703, loss = 1.73233165\n",
      "Iteration 704, loss = 1.73005339\n",
      "Iteration 705, loss = 1.72777370\n",
      "Iteration 706, loss = 1.72549475\n",
      "Iteration 707, loss = 1.72321659\n",
      "Iteration 708, loss = 1.72094029\n",
      "Iteration 709, loss = 1.71866398\n",
      "Iteration 710, loss = 1.71638920\n",
      "Iteration 711, loss = 1.71411601\n",
      "Iteration 712, loss = 1.71184365\n",
      "Iteration 713, loss = 1.70957225\n",
      "Iteration 714, loss = 1.70730177\n",
      "Iteration 715, loss = 1.70503087\n",
      "Iteration 716, loss = 1.70276059\n",
      "Iteration 717, loss = 1.70049215\n",
      "Iteration 718, loss = 1.69822317\n",
      "Iteration 719, loss = 1.69595508\n",
      "Iteration 720, loss = 1.69368808\n",
      "Iteration 721, loss = 1.69142115\n",
      "Iteration 722, loss = 1.68915454\n",
      "Iteration 723, loss = 1.68688905\n",
      "Iteration 724, loss = 1.68462382\n",
      "Iteration 725, loss = 1.68235912\n",
      "Iteration 726, loss = 1.68009483\n",
      "Iteration 727, loss = 1.67782999\n",
      "Iteration 728, loss = 1.67556452\n",
      "Iteration 729, loss = 1.67330026\n",
      "Iteration 730, loss = 1.67103664\n",
      "Iteration 731, loss = 1.66877359\n",
      "Iteration 732, loss = 1.66651133\n",
      "Iteration 733, loss = 1.66424932\n",
      "Iteration 734, loss = 1.66198852\n",
      "Iteration 735, loss = 1.65972881\n",
      "Iteration 736, loss = 1.65746980\n",
      "Iteration 737, loss = 1.65521114\n",
      "Iteration 738, loss = 1.65295325\n",
      "Iteration 739, loss = 1.65069576\n",
      "Iteration 740, loss = 1.64843987\n",
      "Iteration 741, loss = 1.64618549\n",
      "Iteration 742, loss = 1.64393158\n",
      "Iteration 743, loss = 1.64167963\n",
      "Iteration 744, loss = 1.63942844\n",
      "Iteration 745, loss = 1.63717677\n",
      "Iteration 746, loss = 1.63492718\n",
      "Iteration 747, loss = 1.63267922\n",
      "Iteration 748, loss = 1.63043238\n",
      "Iteration 749, loss = 1.62818692\n",
      "Iteration 750, loss = 1.62594268\n",
      "Iteration 751, loss = 1.62369946\n",
      "Iteration 752, loss = 1.62145775\n",
      "Iteration 753, loss = 1.61921727\n",
      "Iteration 754, loss = 1.61697783\n",
      "Iteration 755, loss = 1.61473897\n",
      "Iteration 756, loss = 1.61250146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 757, loss = 1.61026492\n",
      "Iteration 758, loss = 1.60802910\n",
      "Iteration 759, loss = 1.60579481\n",
      "Iteration 760, loss = 1.60356179\n",
      "Iteration 761, loss = 1.60133039\n",
      "Iteration 762, loss = 1.59910050\n",
      "Iteration 763, loss = 1.59687196\n",
      "Iteration 764, loss = 1.59464528\n",
      "Iteration 765, loss = 1.59242055\n",
      "Iteration 766, loss = 1.59019718\n",
      "Iteration 767, loss = 1.58797479\n",
      "Iteration 768, loss = 1.58575327\n",
      "Iteration 769, loss = 1.58353266\n",
      "Iteration 770, loss = 1.58131249\n",
      "Iteration 771, loss = 1.57909415\n",
      "Iteration 772, loss = 1.57687642\n",
      "Iteration 773, loss = 1.57465889\n",
      "Iteration 774, loss = 1.57244197\n",
      "Iteration 775, loss = 1.57022643\n",
      "Iteration 776, loss = 1.56801191\n",
      "Iteration 777, loss = 1.56579844\n",
      "Iteration 778, loss = 1.56358605\n",
      "Iteration 779, loss = 1.56137524\n",
      "Iteration 780, loss = 1.55916579\n",
      "Iteration 781, loss = 1.55695834\n",
      "Iteration 782, loss = 1.55475140\n",
      "Iteration 783, loss = 1.55254688\n",
      "Iteration 784, loss = 1.55034287\n",
      "Iteration 785, loss = 1.54814022\n",
      "Iteration 786, loss = 1.54593873\n",
      "Iteration 787, loss = 1.54373828\n",
      "Iteration 788, loss = 1.54153819\n",
      "Iteration 789, loss = 1.53934088\n",
      "Iteration 790, loss = 1.53714332\n",
      "Iteration 791, loss = 1.53494786\n",
      "Iteration 792, loss = 1.53275319\n",
      "Iteration 793, loss = 1.53056053\n",
      "Iteration 794, loss = 1.52836843\n",
      "Iteration 795, loss = 1.52617855\n",
      "Iteration 796, loss = 1.52398993\n",
      "Iteration 797, loss = 1.52180324\n",
      "Iteration 798, loss = 1.51961744\n",
      "Iteration 799, loss = 1.51743206\n",
      "Iteration 800, loss = 1.51524672\n",
      "Iteration 801, loss = 1.51306186\n",
      "Iteration 802, loss = 1.51087812\n",
      "Iteration 803, loss = 1.50869594\n",
      "Iteration 804, loss = 1.50651408\n",
      "Iteration 805, loss = 1.50433152\n",
      "Iteration 806, loss = 1.50214997\n",
      "Iteration 807, loss = 1.49996871\n",
      "Iteration 808, loss = 1.49778740\n",
      "Iteration 809, loss = 1.49560881\n",
      "Iteration 810, loss = 1.49343131\n",
      "Iteration 811, loss = 1.49125509\n",
      "Iteration 812, loss = 1.48907975\n",
      "Iteration 813, loss = 1.48690433\n",
      "Iteration 814, loss = 1.48472991\n",
      "Iteration 815, loss = 1.48255712\n",
      "Iteration 816, loss = 1.48038442\n",
      "Iteration 817, loss = 1.47821443\n",
      "Iteration 818, loss = 1.47604668\n",
      "Iteration 819, loss = 1.47388031\n",
      "Iteration 820, loss = 1.47171491\n",
      "Iteration 821, loss = 1.46955110\n",
      "Iteration 822, loss = 1.46738894\n",
      "Iteration 823, loss = 1.46522810\n",
      "Iteration 824, loss = 1.46306918\n",
      "Iteration 825, loss = 1.46091223\n",
      "Iteration 826, loss = 1.45875754\n",
      "Iteration 827, loss = 1.45660483\n",
      "Iteration 828, loss = 1.45445378\n",
      "Iteration 829, loss = 1.45230496\n",
      "Iteration 830, loss = 1.45015764\n",
      "Iteration 831, loss = 1.44801185\n",
      "Iteration 832, loss = 1.44586880\n",
      "Iteration 833, loss = 1.44372801\n",
      "Iteration 834, loss = 1.44158910\n",
      "Iteration 835, loss = 1.43945160\n",
      "Iteration 836, loss = 1.43731429\n",
      "Iteration 837, loss = 1.43517901\n",
      "Iteration 838, loss = 1.43304564\n",
      "Iteration 839, loss = 1.43091502\n",
      "Iteration 840, loss = 1.42878732\n",
      "Iteration 841, loss = 1.42666072\n",
      "Iteration 842, loss = 1.42453608\n",
      "Iteration 843, loss = 1.42241281\n",
      "Iteration 844, loss = 1.42029183\n",
      "Iteration 845, loss = 1.41817322\n",
      "Iteration 846, loss = 1.41605665\n",
      "Iteration 847, loss = 1.41394084\n",
      "Iteration 848, loss = 1.41182686\n",
      "Iteration 849, loss = 1.40971361\n",
      "Iteration 850, loss = 1.40760205\n",
      "Iteration 851, loss = 1.40549120\n",
      "Iteration 852, loss = 1.40338172\n",
      "Iteration 853, loss = 1.40127428\n",
      "Iteration 854, loss = 1.39916867\n",
      "Iteration 855, loss = 1.39706498\n",
      "Iteration 856, loss = 1.39496393\n",
      "Iteration 857, loss = 1.39286484\n",
      "Iteration 858, loss = 1.39076807\n",
      "Iteration 859, loss = 1.38867351\n",
      "Iteration 860, loss = 1.38658141\n",
      "Iteration 861, loss = 1.38449136\n",
      "Iteration 862, loss = 1.38240375\n",
      "Iteration 863, loss = 1.38031787\n",
      "Iteration 864, loss = 1.37823455\n",
      "Iteration 865, loss = 1.37615251\n",
      "Iteration 866, loss = 1.37407185\n",
      "Iteration 867, loss = 1.37199249\n",
      "Iteration 868, loss = 1.36991431\n",
      "Iteration 869, loss = 1.36783783\n",
      "Iteration 870, loss = 1.36576332\n",
      "Iteration 871, loss = 1.36369107\n",
      "Iteration 872, loss = 1.36162042\n",
      "Iteration 873, loss = 1.35955152\n",
      "Iteration 874, loss = 1.35748375\n",
      "Iteration 875, loss = 1.35541786\n",
      "Iteration 876, loss = 1.35335393\n",
      "Iteration 877, loss = 1.35129132\n",
      "Iteration 878, loss = 1.34923019\n",
      "Iteration 879, loss = 1.34717083\n",
      "Iteration 880, loss = 1.34511284\n",
      "Iteration 881, loss = 1.34305758\n",
      "Iteration 882, loss = 1.34100426\n",
      "Iteration 883, loss = 1.33895400\n",
      "Iteration 884, loss = 1.33690563\n",
      "Iteration 885, loss = 1.33485943\n",
      "Iteration 886, loss = 1.33281685\n",
      "Iteration 887, loss = 1.33077565\n",
      "Iteration 888, loss = 1.32873618\n",
      "Iteration 889, loss = 1.32669814\n",
      "Iteration 890, loss = 1.32466161\n",
      "Iteration 891, loss = 1.32262670\n",
      "Iteration 892, loss = 1.32059448\n",
      "Iteration 893, loss = 1.31856401\n",
      "Iteration 894, loss = 1.31653575\n",
      "Iteration 895, loss = 1.31450966\n",
      "Iteration 896, loss = 1.31248594\n",
      "Iteration 897, loss = 1.31046424\n",
      "Iteration 898, loss = 1.30844524\n",
      "Iteration 899, loss = 1.30642791\n",
      "Iteration 900, loss = 1.30441261\n",
      "Iteration 901, loss = 1.30239865\n",
      "Iteration 902, loss = 1.30038700\n",
      "Iteration 903, loss = 1.29837804\n",
      "Iteration 904, loss = 1.29637132\n",
      "Iteration 905, loss = 1.29436688\n",
      "Iteration 906, loss = 1.29236457\n",
      "Iteration 907, loss = 1.29036508\n",
      "Iteration 908, loss = 1.28836650\n",
      "Iteration 909, loss = 1.28636953\n",
      "Iteration 910, loss = 1.28437473\n",
      "Iteration 911, loss = 1.28238154\n",
      "Iteration 912, loss = 1.28039103\n",
      "Iteration 913, loss = 1.27840180\n",
      "Iteration 914, loss = 1.27641454\n",
      "Iteration 915, loss = 1.27442946\n",
      "Iteration 916, loss = 1.27244582\n",
      "Iteration 917, loss = 1.27046417\n",
      "Iteration 918, loss = 1.26848508\n",
      "Iteration 919, loss = 1.26650799\n",
      "Iteration 920, loss = 1.26453334\n",
      "Iteration 921, loss = 1.26256042\n",
      "Iteration 922, loss = 1.26058922\n",
      "Iteration 923, loss = 1.25861955\n",
      "Iteration 924, loss = 1.25665152\n",
      "Iteration 925, loss = 1.25468514\n",
      "Iteration 926, loss = 1.25272113\n",
      "Iteration 927, loss = 1.25075834\n",
      "Iteration 928, loss = 1.24879816\n",
      "Iteration 929, loss = 1.24683979\n",
      "Iteration 930, loss = 1.24488319\n",
      "Iteration 931, loss = 1.24292840\n",
      "Iteration 932, loss = 1.24097612\n",
      "Iteration 933, loss = 1.23902576\n",
      "Iteration 934, loss = 1.23707839\n",
      "Iteration 935, loss = 1.23513391\n",
      "Iteration 936, loss = 1.23319251\n",
      "Iteration 937, loss = 1.23125355\n",
      "Iteration 938, loss = 1.22931720\n",
      "Iteration 939, loss = 1.22738298\n",
      "Iteration 940, loss = 1.22545086\n",
      "Iteration 941, loss = 1.22352092\n",
      "Iteration 942, loss = 1.22159284\n",
      "Iteration 943, loss = 1.21966716\n",
      "Iteration 944, loss = 1.21774291\n",
      "Iteration 945, loss = 1.21582133\n",
      "Iteration 946, loss = 1.21390226\n",
      "Iteration 947, loss = 1.21198454\n",
      "Iteration 948, loss = 1.21006962\n",
      "Iteration 949, loss = 1.20815694\n",
      "Iteration 950, loss = 1.20624638\n",
      "Iteration 951, loss = 1.20433788\n",
      "Iteration 952, loss = 1.20243168\n",
      "Iteration 953, loss = 1.20052829\n",
      "Iteration 954, loss = 1.19862651\n",
      "Iteration 955, loss = 1.19672719\n",
      "Iteration 956, loss = 1.19483040\n",
      "Iteration 957, loss = 1.19293609\n",
      "Iteration 958, loss = 1.19104304\n",
      "Iteration 959, loss = 1.18915249\n",
      "Iteration 960, loss = 1.18726433\n",
      "Iteration 961, loss = 1.18537816\n",
      "Iteration 962, loss = 1.18349438\n",
      "Iteration 963, loss = 1.18161350\n",
      "Iteration 964, loss = 1.17973495\n",
      "Iteration 965, loss = 1.17785906\n",
      "Iteration 966, loss = 1.17598580\n",
      "Iteration 967, loss = 1.17411492\n",
      "Iteration 968, loss = 1.17224610\n",
      "Iteration 969, loss = 1.17037901\n",
      "Iteration 970, loss = 1.16851353\n",
      "Iteration 971, loss = 1.16665052\n",
      "Iteration 972, loss = 1.16479005\n",
      "Iteration 973, loss = 1.16293244\n",
      "Iteration 974, loss = 1.16107617\n",
      "Iteration 975, loss = 1.15922337\n",
      "Iteration 976, loss = 1.15737257\n",
      "Iteration 977, loss = 1.15552390\n",
      "Iteration 978, loss = 1.15367793\n",
      "Iteration 979, loss = 1.15183464\n",
      "Iteration 980, loss = 1.14999432\n",
      "Iteration 981, loss = 1.14815696\n",
      "Iteration 982, loss = 1.14632284\n",
      "Iteration 983, loss = 1.14449080\n",
      "Iteration 984, loss = 1.14266113\n",
      "Iteration 985, loss = 1.14083442\n",
      "Iteration 986, loss = 1.13900985\n",
      "Iteration 987, loss = 1.13718771\n",
      "Iteration 988, loss = 1.13536838\n",
      "Iteration 989, loss = 1.13355106\n",
      "Iteration 990, loss = 1.13173666\n",
      "Iteration 991, loss = 1.12992404\n",
      "Iteration 992, loss = 1.12811443\n",
      "Iteration 993, loss = 1.12630696\n",
      "Iteration 994, loss = 1.12450206\n",
      "Iteration 995, loss = 1.12269949\n",
      "Iteration 996, loss = 1.12089916\n",
      "Iteration 997, loss = 1.11910125\n",
      "Iteration 998, loss = 1.11730580\n",
      "Iteration 999, loss = 1.11551318\n",
      "Iteration 1000, loss = 1.11372241\n",
      "Iteration 1001, loss = 1.11193439\n",
      "Iteration 1002, loss = 1.11014923\n",
      "Iteration 1003, loss = 1.10836622\n",
      "Iteration 1004, loss = 1.10658630\n",
      "Iteration 1005, loss = 1.10480757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1006, loss = 1.10303174\n",
      "Iteration 1007, loss = 1.10125827\n",
      "Iteration 1008, loss = 1.09948723\n",
      "Iteration 1009, loss = 1.09771893\n",
      "Iteration 1010, loss = 1.09595325\n",
      "Iteration 1011, loss = 1.09419028\n",
      "Iteration 1012, loss = 1.09242922\n",
      "Iteration 1013, loss = 1.09067079\n",
      "Iteration 1014, loss = 1.08891443\n",
      "Iteration 1015, loss = 1.08716064\n",
      "Iteration 1016, loss = 1.08540907\n",
      "Iteration 1017, loss = 1.08366003\n",
      "Iteration 1018, loss = 1.08191298\n",
      "Iteration 1019, loss = 1.08016896\n",
      "Iteration 1020, loss = 1.07842669\n",
      "Iteration 1021, loss = 1.07668693\n",
      "Iteration 1022, loss = 1.07494946\n",
      "Iteration 1023, loss = 1.07321436\n",
      "Iteration 1024, loss = 1.07148197\n",
      "Iteration 1025, loss = 1.06975130\n",
      "Iteration 1026, loss = 1.06802324\n",
      "Iteration 1027, loss = 1.06629780\n",
      "Iteration 1028, loss = 1.06457422\n",
      "Iteration 1029, loss = 1.06285316\n",
      "Iteration 1030, loss = 1.06113465\n",
      "Iteration 1031, loss = 1.05941864\n",
      "Iteration 1032, loss = 1.05770437\n",
      "Iteration 1033, loss = 1.05599303\n",
      "Iteration 1034, loss = 1.05428367\n",
      "Iteration 1035, loss = 1.05257699\n",
      "Iteration 1036, loss = 1.05087283\n",
      "Iteration 1037, loss = 1.04917089\n",
      "Iteration 1038, loss = 1.04747129\n",
      "Iteration 1039, loss = 1.04577420\n",
      "Iteration 1040, loss = 1.04407944\n",
      "Iteration 1041, loss = 1.04238709\n",
      "Iteration 1042, loss = 1.04069674\n",
      "Iteration 1043, loss = 1.03900884\n",
      "Iteration 1044, loss = 1.03732349\n",
      "Iteration 1045, loss = 1.03564018\n",
      "Iteration 1046, loss = 1.03395983\n",
      "Iteration 1047, loss = 1.03228179\n",
      "Iteration 1048, loss = 1.03060573\n",
      "Iteration 1049, loss = 1.02893259\n",
      "Iteration 1050, loss = 1.02726197\n",
      "Iteration 1051, loss = 1.02559394\n",
      "Iteration 1052, loss = 1.02392791\n",
      "Iteration 1053, loss = 1.02226417\n",
      "Iteration 1054, loss = 1.02060308\n",
      "Iteration 1055, loss = 1.01894419\n",
      "Iteration 1056, loss = 1.01728821\n",
      "Iteration 1057, loss = 1.01563441\n",
      "Iteration 1058, loss = 1.01398326\n",
      "Iteration 1059, loss = 1.01233490\n",
      "Iteration 1060, loss = 1.01068862\n",
      "Iteration 1061, loss = 1.00904503\n",
      "Iteration 1062, loss = 1.00740344\n",
      "Iteration 1063, loss = 1.00576471\n",
      "Iteration 1064, loss = 1.00412856\n",
      "Iteration 1065, loss = 1.00249487\n",
      "Iteration 1066, loss = 1.00086325\n",
      "Iteration 1067, loss = 0.99923460\n",
      "Iteration 1068, loss = 0.99760833\n",
      "Iteration 1069, loss = 0.99598418\n",
      "Iteration 1070, loss = 0.99436286\n",
      "Iteration 1071, loss = 0.99274409\n",
      "Iteration 1072, loss = 0.99112757\n",
      "Iteration 1073, loss = 0.98951300\n",
      "Iteration 1074, loss = 0.98790068\n",
      "Iteration 1075, loss = 0.98629081\n",
      "Iteration 1076, loss = 0.98468344\n",
      "Iteration 1077, loss = 0.98307806\n",
      "Iteration 1078, loss = 0.98147515\n",
      "Iteration 1079, loss = 0.97987456\n",
      "Iteration 1080, loss = 0.97827639\n",
      "Iteration 1081, loss = 0.97668049\n",
      "Iteration 1082, loss = 0.97508676\n",
      "Iteration 1083, loss = 0.97349555\n",
      "Iteration 1084, loss = 0.97190694\n",
      "Iteration 1085, loss = 0.97032128\n",
      "Iteration 1086, loss = 0.96873782\n",
      "Iteration 1087, loss = 0.96715656\n",
      "Iteration 1088, loss = 0.96557736\n",
      "Iteration 1089, loss = 0.96400035\n",
      "Iteration 1090, loss = 0.96242539\n",
      "Iteration 1091, loss = 0.96085228\n",
      "Iteration 1092, loss = 0.95928133\n",
      "Iteration 1093, loss = 0.95771281\n",
      "Iteration 1094, loss = 0.95614656\n",
      "Iteration 1095, loss = 0.95458215\n",
      "Iteration 1096, loss = 0.95301925\n",
      "Iteration 1097, loss = 0.95145846\n",
      "Iteration 1098, loss = 0.94990030\n",
      "Iteration 1099, loss = 0.94834408\n",
      "Iteration 1100, loss = 0.94679057\n",
      "Iteration 1101, loss = 0.94523886\n",
      "Iteration 1102, loss = 0.94368977\n",
      "Iteration 1103, loss = 0.94214285\n",
      "Iteration 1104, loss = 0.94059814\n",
      "Iteration 1105, loss = 0.93905612\n",
      "Iteration 1106, loss = 0.93751614\n",
      "Iteration 1107, loss = 0.93597864\n",
      "Iteration 1108, loss = 0.93444390\n",
      "Iteration 1109, loss = 0.93291137\n",
      "Iteration 1110, loss = 0.93138112\n",
      "Iteration 1111, loss = 0.92985353\n",
      "Iteration 1112, loss = 0.92832838\n",
      "Iteration 1113, loss = 0.92680490\n",
      "Iteration 1114, loss = 0.92528431\n",
      "Iteration 1115, loss = 0.92376648\n",
      "Iteration 1116, loss = 0.92225216\n",
      "Iteration 1117, loss = 0.92074088\n",
      "Iteration 1118, loss = 0.91923138\n",
      "Iteration 1119, loss = 0.91772417\n",
      "Iteration 1120, loss = 0.91621918\n",
      "Iteration 1121, loss = 0.91471615\n",
      "Iteration 1122, loss = 0.91321611\n",
      "Iteration 1123, loss = 0.91171877\n",
      "Iteration 1124, loss = 0.91022305\n",
      "Iteration 1125, loss = 0.90872924\n",
      "Iteration 1126, loss = 0.90723772\n",
      "Iteration 1127, loss = 0.90574861\n",
      "Iteration 1128, loss = 0.90426209\n",
      "Iteration 1129, loss = 0.90277767\n",
      "Iteration 1130, loss = 0.90129652\n",
      "Iteration 1131, loss = 0.89981845\n",
      "Iteration 1132, loss = 0.89834314\n",
      "Iteration 1133, loss = 0.89687018\n",
      "Iteration 1134, loss = 0.89539970\n",
      "Iteration 1135, loss = 0.89393280\n",
      "Iteration 1136, loss = 0.89246875\n",
      "Iteration 1137, loss = 0.89100754\n",
      "Iteration 1138, loss = 0.88954860\n",
      "Iteration 1139, loss = 0.88809196\n",
      "Iteration 1140, loss = 0.88663772\n",
      "Iteration 1141, loss = 0.88518579\n",
      "Iteration 1142, loss = 0.88373584\n",
      "Iteration 1143, loss = 0.88228934\n",
      "Iteration 1144, loss = 0.88084548\n",
      "Iteration 1145, loss = 0.87940428\n",
      "Iteration 1146, loss = 0.87796521\n",
      "Iteration 1147, loss = 0.87652848\n",
      "Iteration 1148, loss = 0.87509401\n",
      "Iteration 1149, loss = 0.87366207\n",
      "Iteration 1150, loss = 0.87223255\n",
      "Iteration 1151, loss = 0.87080513\n",
      "Iteration 1152, loss = 0.86938001\n",
      "Iteration 1153, loss = 0.86795728\n",
      "Iteration 1154, loss = 0.86653674\n",
      "Iteration 1155, loss = 0.86511911\n",
      "Iteration 1156, loss = 0.86370446\n",
      "Iteration 1157, loss = 0.86229157\n",
      "Iteration 1158, loss = 0.86088121\n",
      "Iteration 1159, loss = 0.85947327\n",
      "Iteration 1160, loss = 0.85806800\n",
      "Iteration 1161, loss = 0.85666496\n",
      "Iteration 1162, loss = 0.85526461\n",
      "Iteration 1163, loss = 0.85386715\n",
      "Iteration 1164, loss = 0.85247192\n",
      "Iteration 1165, loss = 0.85107979\n",
      "Iteration 1166, loss = 0.84968986\n",
      "Iteration 1167, loss = 0.84830256\n",
      "Iteration 1168, loss = 0.84691771\n",
      "Iteration 1169, loss = 0.84553501\n",
      "Iteration 1170, loss = 0.84415468\n",
      "Iteration 1171, loss = 0.84277646\n",
      "Iteration 1172, loss = 0.84140071\n",
      "Iteration 1173, loss = 0.84002730\n",
      "Iteration 1174, loss = 0.83865611\n",
      "Iteration 1175, loss = 0.83728762\n",
      "Iteration 1176, loss = 0.83592136\n",
      "Iteration 1177, loss = 0.83455706\n",
      "Iteration 1178, loss = 0.83319471\n",
      "Iteration 1179, loss = 0.83183493\n",
      "Iteration 1180, loss = 0.83047739\n",
      "Iteration 1181, loss = 0.82912217\n",
      "Iteration 1182, loss = 0.82776898\n",
      "Iteration 1183, loss = 0.82641808\n",
      "Iteration 1184, loss = 0.82506940\n",
      "Iteration 1185, loss = 0.82372346\n",
      "Iteration 1186, loss = 0.82237968\n",
      "Iteration 1187, loss = 0.82103808\n",
      "Iteration 1188, loss = 0.81969911\n",
      "Iteration 1189, loss = 0.81836209\n",
      "Iteration 1190, loss = 0.81702757\n",
      "Iteration 1191, loss = 0.81569534\n",
      "Iteration 1192, loss = 0.81436513\n",
      "Iteration 1193, loss = 0.81303736\n",
      "Iteration 1194, loss = 0.81171204\n",
      "Iteration 1195, loss = 0.81038916\n",
      "Iteration 1196, loss = 0.80906924\n",
      "Iteration 1197, loss = 0.80775123\n",
      "Iteration 1198, loss = 0.80643536\n",
      "Iteration 1199, loss = 0.80512207\n",
      "Iteration 1200, loss = 0.80381100\n",
      "Iteration 1201, loss = 0.80250291\n",
      "Iteration 1202, loss = 0.80119685\n",
      "Iteration 1203, loss = 0.79989333\n",
      "Iteration 1204, loss = 0.79859235\n",
      "Iteration 1205, loss = 0.79729433\n",
      "Iteration 1206, loss = 0.79599845\n",
      "Iteration 1207, loss = 0.79470503\n",
      "Iteration 1208, loss = 0.79341352\n",
      "Iteration 1209, loss = 0.79212431\n",
      "Iteration 1210, loss = 0.79083733\n",
      "Iteration 1211, loss = 0.78955294\n",
      "Iteration 1212, loss = 0.78827120\n",
      "Iteration 1213, loss = 0.78699231\n",
      "Iteration 1214, loss = 0.78571563\n",
      "Iteration 1215, loss = 0.78444149\n",
      "Iteration 1216, loss = 0.78316927\n",
      "Iteration 1217, loss = 0.78189925\n",
      "Iteration 1218, loss = 0.78063185\n",
      "Iteration 1219, loss = 0.77936639\n",
      "Iteration 1220, loss = 0.77810332\n",
      "Iteration 1221, loss = 0.77684268\n",
      "Iteration 1222, loss = 0.77558437\n",
      "Iteration 1223, loss = 0.77432796\n",
      "Iteration 1224, loss = 0.77307387\n",
      "Iteration 1225, loss = 0.77182171\n",
      "Iteration 1226, loss = 0.77057117\n",
      "Iteration 1227, loss = 0.76932292\n",
      "Iteration 1228, loss = 0.76807615\n",
      "Iteration 1229, loss = 0.76683176\n",
      "Iteration 1230, loss = 0.76558969\n",
      "Iteration 1231, loss = 0.76434896\n",
      "Iteration 1232, loss = 0.76311057\n",
      "Iteration 1233, loss = 0.76187455\n",
      "Iteration 1234, loss = 0.76064099\n",
      "Iteration 1235, loss = 0.75941038\n",
      "Iteration 1236, loss = 0.75818147\n",
      "Iteration 1237, loss = 0.75695472\n",
      "Iteration 1238, loss = 0.75573052\n",
      "Iteration 1239, loss = 0.75450804\n",
      "Iteration 1240, loss = 0.75328817\n",
      "Iteration 1241, loss = 0.75207036\n",
      "Iteration 1242, loss = 0.75085451\n",
      "Iteration 1243, loss = 0.74964095\n",
      "Iteration 1244, loss = 0.74842950\n",
      "Iteration 1245, loss = 0.74722086\n",
      "Iteration 1246, loss = 0.74601426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1247, loss = 0.74481023\n",
      "Iteration 1248, loss = 0.74360837\n",
      "Iteration 1249, loss = 0.74240890\n",
      "Iteration 1250, loss = 0.74121150\n",
      "Iteration 1251, loss = 0.74001623\n",
      "Iteration 1252, loss = 0.73882287\n",
      "Iteration 1253, loss = 0.73763177\n",
      "Iteration 1254, loss = 0.73644260\n",
      "Iteration 1255, loss = 0.73525554\n",
      "Iteration 1256, loss = 0.73407083\n",
      "Iteration 1257, loss = 0.73288807\n",
      "Iteration 1258, loss = 0.73170716\n",
      "Iteration 1259, loss = 0.73052869\n",
      "Iteration 1260, loss = 0.72935217\n",
      "Iteration 1261, loss = 0.72817828\n",
      "Iteration 1262, loss = 0.72700630\n",
      "Iteration 1263, loss = 0.72583691\n",
      "Iteration 1264, loss = 0.72467002\n",
      "Iteration 1265, loss = 0.72350591\n",
      "Iteration 1266, loss = 0.72234378\n",
      "Iteration 1267, loss = 0.72118367\n",
      "Iteration 1268, loss = 0.72002610\n",
      "Iteration 1269, loss = 0.71887023\n",
      "Iteration 1270, loss = 0.71771675\n",
      "Iteration 1271, loss = 0.71656622\n",
      "Iteration 1272, loss = 0.71541715\n",
      "Iteration 1273, loss = 0.71427065\n",
      "Iteration 1274, loss = 0.71312626\n",
      "Iteration 1275, loss = 0.71198392\n",
      "Iteration 1276, loss = 0.71084374\n",
      "Iteration 1277, loss = 0.70970541\n",
      "Iteration 1278, loss = 0.70856938\n",
      "Iteration 1279, loss = 0.70743524\n",
      "Iteration 1280, loss = 0.70630349\n",
      "Iteration 1281, loss = 0.70517361\n",
      "Iteration 1282, loss = 0.70404567\n",
      "Iteration 1283, loss = 0.70291998\n",
      "Iteration 1284, loss = 0.70179616\n",
      "Iteration 1285, loss = 0.70067437\n",
      "Iteration 1286, loss = 0.69955446\n",
      "Iteration 1287, loss = 0.69843674\n",
      "Iteration 1288, loss = 0.69732118\n",
      "Iteration 1289, loss = 0.69620779\n",
      "Iteration 1290, loss = 0.69509644\n",
      "Iteration 1291, loss = 0.69398710\n",
      "Iteration 1292, loss = 0.69287973\n",
      "Iteration 1293, loss = 0.69177447\n",
      "Iteration 1294, loss = 0.69067152\n",
      "Iteration 1295, loss = 0.68957052\n",
      "Iteration 1296, loss = 0.68847175\n",
      "Iteration 1297, loss = 0.68737521\n",
      "Iteration 1298, loss = 0.68628120\n",
      "Iteration 1299, loss = 0.68518899\n",
      "Iteration 1300, loss = 0.68409880\n",
      "Iteration 1301, loss = 0.68301121\n",
      "Iteration 1302, loss = 0.68192561\n",
      "Iteration 1303, loss = 0.68084213\n",
      "Iteration 1304, loss = 0.67976102\n",
      "Iteration 1305, loss = 0.67868207\n",
      "Iteration 1306, loss = 0.67760526\n",
      "Iteration 1307, loss = 0.67653047\n",
      "Iteration 1308, loss = 0.67545771\n",
      "Iteration 1309, loss = 0.67438715\n",
      "Iteration 1310, loss = 0.67331829\n",
      "Iteration 1311, loss = 0.67225173\n",
      "Iteration 1312, loss = 0.67118721\n",
      "Iteration 1313, loss = 0.67012474\n",
      "Iteration 1314, loss = 0.66906435\n",
      "Iteration 1315, loss = 0.66800607\n",
      "Iteration 1316, loss = 0.66694984\n",
      "Iteration 1317, loss = 0.66589562\n",
      "Iteration 1318, loss = 0.66484317\n",
      "Iteration 1319, loss = 0.66379273\n",
      "Iteration 1320, loss = 0.66274433\n",
      "Iteration 1321, loss = 0.66169771\n",
      "Iteration 1322, loss = 0.66065333\n",
      "Iteration 1323, loss = 0.65961079\n",
      "Iteration 1324, loss = 0.65857039\n",
      "Iteration 1325, loss = 0.65753169\n",
      "Iteration 1326, loss = 0.65649507\n",
      "Iteration 1327, loss = 0.65546050\n",
      "Iteration 1328, loss = 0.65442756\n",
      "Iteration 1329, loss = 0.65339651\n",
      "Iteration 1330, loss = 0.65236757\n",
      "Iteration 1331, loss = 0.65134062\n",
      "Iteration 1332, loss = 0.65031536\n",
      "Iteration 1333, loss = 0.64929208\n",
      "Iteration 1334, loss = 0.64827073\n",
      "Iteration 1335, loss = 0.64725119\n",
      "Iteration 1336, loss = 0.64623363\n",
      "Iteration 1337, loss = 0.64521788\n",
      "Iteration 1338, loss = 0.64420400\n",
      "Iteration 1339, loss = 0.64319234\n",
      "Iteration 1340, loss = 0.64218246\n",
      "Iteration 1341, loss = 0.64117432\n",
      "Iteration 1342, loss = 0.64016826\n",
      "Iteration 1343, loss = 0.63916418\n",
      "Iteration 1344, loss = 0.63816189\n",
      "Iteration 1345, loss = 0.63716200\n",
      "Iteration 1346, loss = 0.63616346\n",
      "Iteration 1347, loss = 0.63516696\n",
      "Iteration 1348, loss = 0.63417214\n",
      "Iteration 1349, loss = 0.63317932\n",
      "Iteration 1350, loss = 0.63218828\n",
      "Iteration 1351, loss = 0.63119935\n",
      "Iteration 1352, loss = 0.63021221\n",
      "Iteration 1353, loss = 0.62922687\n",
      "Iteration 1354, loss = 0.62824366\n",
      "Iteration 1355, loss = 0.62726241\n",
      "Iteration 1356, loss = 0.62628282\n",
      "Iteration 1357, loss = 0.62530518\n",
      "Iteration 1358, loss = 0.62432939\n",
      "Iteration 1359, loss = 0.62335538\n",
      "Iteration 1360, loss = 0.62238297\n",
      "Iteration 1361, loss = 0.62141274\n",
      "Iteration 1362, loss = 0.62044402\n",
      "Iteration 1363, loss = 0.61947726\n",
      "Iteration 1364, loss = 0.61851233\n",
      "Iteration 1365, loss = 0.61754910\n",
      "Iteration 1366, loss = 0.61658792\n",
      "Iteration 1367, loss = 0.61562832\n",
      "Iteration 1368, loss = 0.61467072\n",
      "Iteration 1369, loss = 0.61371487\n",
      "Iteration 1370, loss = 0.61276097\n",
      "Iteration 1371, loss = 0.61180873\n",
      "Iteration 1372, loss = 0.61085827\n",
      "Iteration 1373, loss = 0.60990975\n",
      "Iteration 1374, loss = 0.60896277\n",
      "Iteration 1375, loss = 0.60801761\n",
      "Iteration 1376, loss = 0.60707423\n",
      "Iteration 1377, loss = 0.60613273\n",
      "Iteration 1378, loss = 0.60519302\n",
      "Iteration 1379, loss = 0.60425511\n",
      "Iteration 1380, loss = 0.60331919\n",
      "Iteration 1381, loss = 0.60238471\n",
      "Iteration 1382, loss = 0.60145225\n",
      "Iteration 1383, loss = 0.60052148\n",
      "Iteration 1384, loss = 0.59959239\n",
      "Iteration 1385, loss = 0.59866497\n",
      "Iteration 1386, loss = 0.59773939\n",
      "Iteration 1387, loss = 0.59681548\n",
      "Iteration 1388, loss = 0.59589339\n",
      "Iteration 1389, loss = 0.59497309\n",
      "Iteration 1390, loss = 0.59405455\n",
      "Iteration 1391, loss = 0.59313779\n",
      "Iteration 1392, loss = 0.59222293\n",
      "Iteration 1393, loss = 0.59130977\n",
      "Iteration 1394, loss = 0.59039854\n",
      "Iteration 1395, loss = 0.58948905\n",
      "Iteration 1396, loss = 0.58858119\n",
      "Iteration 1397, loss = 0.58767509\n",
      "Iteration 1398, loss = 0.58677083\n",
      "Iteration 1399, loss = 0.58586822\n",
      "Iteration 1400, loss = 0.58496755\n",
      "Iteration 1401, loss = 0.58406821\n",
      "Iteration 1402, loss = 0.58317083\n",
      "Iteration 1403, loss = 0.58227514\n",
      "Iteration 1404, loss = 0.58138111\n",
      "Iteration 1405, loss = 0.58048874\n",
      "Iteration 1406, loss = 0.57959800\n",
      "Iteration 1407, loss = 0.57870894\n",
      "Iteration 1408, loss = 0.57782182\n",
      "Iteration 1409, loss = 0.57693593\n",
      "Iteration 1410, loss = 0.57605187\n",
      "Iteration 1411, loss = 0.57516931\n",
      "Iteration 1412, loss = 0.57428853\n",
      "Iteration 1413, loss = 0.57340917\n",
      "Iteration 1414, loss = 0.57253176\n",
      "Iteration 1415, loss = 0.57165592\n",
      "Iteration 1416, loss = 0.57078170\n",
      "Iteration 1417, loss = 0.56990917\n",
      "Iteration 1418, loss = 0.56903835\n",
      "Iteration 1419, loss = 0.56816922\n",
      "Iteration 1420, loss = 0.56730185\n",
      "Iteration 1421, loss = 0.56643590\n",
      "Iteration 1422, loss = 0.56557173\n",
      "Iteration 1423, loss = 0.56470907\n",
      "Iteration 1424, loss = 0.56384808\n",
      "Iteration 1425, loss = 0.56298865\n",
      "Iteration 1426, loss = 0.56213087\n",
      "Iteration 1427, loss = 0.56127473\n",
      "Iteration 1428, loss = 0.56042022\n",
      "Iteration 1429, loss = 0.55956741\n",
      "Iteration 1430, loss = 0.55871618\n",
      "Iteration 1431, loss = 0.55786681\n",
      "Iteration 1432, loss = 0.55701881\n",
      "Iteration 1433, loss = 0.55617251\n",
      "Iteration 1434, loss = 0.55532767\n",
      "Iteration 1435, loss = 0.55448464\n",
      "Iteration 1436, loss = 0.55364285\n",
      "Iteration 1437, loss = 0.55280265\n",
      "Iteration 1438, loss = 0.55196441\n",
      "Iteration 1439, loss = 0.55112737\n",
      "Iteration 1440, loss = 0.55029211\n",
      "Iteration 1441, loss = 0.54945833\n",
      "Iteration 1442, loss = 0.54862616\n",
      "Iteration 1443, loss = 0.54779576\n",
      "Iteration 1444, loss = 0.54696661\n",
      "Iteration 1445, loss = 0.54613911\n",
      "Iteration 1446, loss = 0.54531303\n",
      "Iteration 1447, loss = 0.54448875\n",
      "Iteration 1448, loss = 0.54366593\n",
      "Iteration 1449, loss = 0.54284476\n",
      "Iteration 1450, loss = 0.54202504\n",
      "Iteration 1451, loss = 0.54120698\n",
      "Iteration 1452, loss = 0.54039035\n",
      "Iteration 1453, loss = 0.53957524\n",
      "Iteration 1454, loss = 0.53876177\n",
      "Iteration 1455, loss = 0.53794992\n",
      "Iteration 1456, loss = 0.53713950\n",
      "Iteration 1457, loss = 0.53633034\n",
      "Iteration 1458, loss = 0.53552300\n",
      "Iteration 1459, loss = 0.53471683\n",
      "Iteration 1460, loss = 0.53391253\n",
      "Iteration 1461, loss = 0.53310961\n",
      "Iteration 1462, loss = 0.53230803\n",
      "Iteration 1463, loss = 0.53150809\n",
      "Iteration 1464, loss = 0.53070916\n",
      "Iteration 1465, loss = 0.52991222\n",
      "Iteration 1466, loss = 0.52911629\n",
      "Iteration 1467, loss = 0.52832200\n",
      "Iteration 1468, loss = 0.52752918\n",
      "Iteration 1469, loss = 0.52673788\n",
      "Iteration 1470, loss = 0.52594810\n",
      "Iteration 1471, loss = 0.52515986\n",
      "Iteration 1472, loss = 0.52437293\n",
      "Iteration 1473, loss = 0.52358746\n",
      "Iteration 1474, loss = 0.52280332\n",
      "Iteration 1475, loss = 0.52202073\n",
      "Iteration 1476, loss = 0.52123939\n",
      "Iteration 1477, loss = 0.52045955\n",
      "Iteration 1478, loss = 0.51968141\n",
      "Iteration 1479, loss = 0.51890465\n",
      "Iteration 1480, loss = 0.51812933\n",
      "Iteration 1481, loss = 0.51735528\n",
      "Iteration 1482, loss = 0.51658251\n",
      "Iteration 1483, loss = 0.51581140\n",
      "Iteration 1484, loss = 0.51504162\n",
      "Iteration 1485, loss = 0.51427311\n",
      "Iteration 1486, loss = 0.51350614\n",
      "Iteration 1487, loss = 0.51274100\n",
      "Iteration 1488, loss = 0.51197703\n",
      "Iteration 1489, loss = 0.51121501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1490, loss = 0.51045475\n",
      "Iteration 1491, loss = 0.50969613\n",
      "Iteration 1492, loss = 0.50893877\n",
      "Iteration 1493, loss = 0.50818304\n",
      "Iteration 1494, loss = 0.50742904\n",
      "Iteration 1495, loss = 0.50667623\n",
      "Iteration 1496, loss = 0.50592503\n",
      "Iteration 1497, loss = 0.50517529\n",
      "Iteration 1498, loss = 0.50442686\n",
      "Iteration 1499, loss = 0.50367987\n",
      "Iteration 1500, loss = 0.50293438\n",
      "Iteration 1501, loss = 0.50219008\n",
      "Iteration 1502, loss = 0.50144733\n",
      "Iteration 1503, loss = 0.50070617\n",
      "Iteration 1504, loss = 0.49996609\n",
      "Iteration 1505, loss = 0.49922775\n",
      "Iteration 1506, loss = 0.49849063\n",
      "Iteration 1507, loss = 0.49775505\n",
      "Iteration 1508, loss = 0.49702061\n",
      "Iteration 1509, loss = 0.49628787\n",
      "Iteration 1510, loss = 0.49555616\n",
      "Iteration 1511, loss = 0.49482601\n",
      "Iteration 1512, loss = 0.49409708\n",
      "Iteration 1513, loss = 0.49336946\n",
      "Iteration 1514, loss = 0.49264314\n",
      "Iteration 1515, loss = 0.49191824\n",
      "Iteration 1516, loss = 0.49119462\n",
      "Iteration 1517, loss = 0.49047231\n",
      "Iteration 1518, loss = 0.48975130\n",
      "Iteration 1519, loss = 0.48903174\n",
      "Iteration 1520, loss = 0.48831353\n",
      "Iteration 1521, loss = 0.48759659\n",
      "Iteration 1522, loss = 0.48688106\n",
      "Iteration 1523, loss = 0.48616681\n",
      "Iteration 1524, loss = 0.48545395\n",
      "Iteration 1525, loss = 0.48474228\n",
      "Iteration 1526, loss = 0.48403211\n",
      "Iteration 1527, loss = 0.48332336\n",
      "Iteration 1528, loss = 0.48261581\n",
      "Iteration 1529, loss = 0.48190951\n",
      "Iteration 1530, loss = 0.48120476\n",
      "Iteration 1531, loss = 0.48050118\n",
      "Iteration 1532, loss = 0.47979896\n",
      "Iteration 1533, loss = 0.47909793\n",
      "Iteration 1534, loss = 0.47839813\n",
      "Iteration 1535, loss = 0.47769981\n",
      "Iteration 1536, loss = 0.47700283\n",
      "Iteration 1537, loss = 0.47630690\n",
      "Iteration 1538, loss = 0.47561252\n",
      "Iteration 1539, loss = 0.47491928\n",
      "Iteration 1540, loss = 0.47422754\n",
      "Iteration 1541, loss = 0.47353692\n",
      "Iteration 1542, loss = 0.47284767\n",
      "Iteration 1543, loss = 0.47215971\n",
      "Iteration 1544, loss = 0.47147314\n",
      "Iteration 1545, loss = 0.47078785\n",
      "Iteration 1546, loss = 0.47010374\n",
      "Iteration 1547, loss = 0.46942118\n",
      "Iteration 1548, loss = 0.46873973\n",
      "Iteration 1549, loss = 0.46805946\n",
      "Iteration 1550, loss = 0.46738069\n",
      "Iteration 1551, loss = 0.46670314\n",
      "Iteration 1552, loss = 0.46602691\n",
      "Iteration 1553, loss = 0.46535186\n",
      "Iteration 1554, loss = 0.46467793\n",
      "Iteration 1555, loss = 0.46400532\n",
      "Iteration 1556, loss = 0.46333411\n",
      "Iteration 1557, loss = 0.46266397\n",
      "Iteration 1558, loss = 0.46199512\n",
      "Iteration 1559, loss = 0.46132767\n",
      "Iteration 1560, loss = 0.46066135\n",
      "Iteration 1561, loss = 0.45999627\n",
      "Iteration 1562, loss = 0.45933242\n",
      "Iteration 1563, loss = 0.45866973\n",
      "Iteration 1564, loss = 0.45800834\n",
      "Iteration 1565, loss = 0.45734800\n",
      "Iteration 1566, loss = 0.45668906\n",
      "Iteration 1567, loss = 0.45603119\n",
      "Iteration 1568, loss = 0.45537461\n",
      "Iteration 1569, loss = 0.45471922\n",
      "Iteration 1570, loss = 0.45406511\n",
      "Iteration 1571, loss = 0.45341212\n",
      "Iteration 1572, loss = 0.45276040\n",
      "Iteration 1573, loss = 0.45210962\n",
      "Iteration 1574, loss = 0.45146017\n",
      "Iteration 1575, loss = 0.45081189\n",
      "Iteration 1576, loss = 0.45016478\n",
      "Iteration 1577, loss = 0.44951899\n",
      "Iteration 1578, loss = 0.44887434\n",
      "Iteration 1579, loss = 0.44823088\n",
      "Iteration 1580, loss = 0.44758861\n",
      "Iteration 1581, loss = 0.44694748\n",
      "Iteration 1582, loss = 0.44630757\n",
      "Iteration 1583, loss = 0.44566889\n",
      "Iteration 1584, loss = 0.44503128\n",
      "Iteration 1585, loss = 0.44439487\n",
      "Iteration 1586, loss = 0.44375985\n",
      "Iteration 1587, loss = 0.44312578\n",
      "Iteration 1588, loss = 0.44249297\n",
      "Iteration 1589, loss = 0.44186123\n",
      "Iteration 1590, loss = 0.44123074\n",
      "Iteration 1591, loss = 0.44060122\n",
      "Iteration 1592, loss = 0.43997285\n",
      "Iteration 1593, loss = 0.43934567\n",
      "Iteration 1594, loss = 0.43871952\n",
      "Iteration 1595, loss = 0.43809468\n",
      "Iteration 1596, loss = 0.43747087\n",
      "Iteration 1597, loss = 0.43684824\n",
      "Iteration 1598, loss = 0.43622676\n",
      "Iteration 1599, loss = 0.43560640\n",
      "Iteration 1600, loss = 0.43498719\n",
      "Iteration 1601, loss = 0.43436894\n",
      "Iteration 1602, loss = 0.43375196\n",
      "Iteration 1603, loss = 0.43313606\n",
      "Iteration 1604, loss = 0.43252126\n",
      "Iteration 1605, loss = 0.43190773\n",
      "Iteration 1606, loss = 0.43129528\n",
      "Iteration 1607, loss = 0.43068409\n",
      "Iteration 1608, loss = 0.43007385\n",
      "Iteration 1609, loss = 0.42946489\n",
      "Iteration 1610, loss = 0.42885698\n",
      "Iteration 1611, loss = 0.42825040\n",
      "Iteration 1612, loss = 0.42764477\n",
      "Iteration 1613, loss = 0.42704038\n",
      "Iteration 1614, loss = 0.42643717\n",
      "Iteration 1615, loss = 0.42583507\n",
      "Iteration 1616, loss = 0.42523410\n",
      "Iteration 1617, loss = 0.42463423\n",
      "Iteration 1618, loss = 0.42403543\n",
      "Iteration 1619, loss = 0.42343781\n",
      "Iteration 1620, loss = 0.42284132\n",
      "Iteration 1621, loss = 0.42224582\n",
      "Iteration 1622, loss = 0.42165152\n",
      "Iteration 1623, loss = 0.42105822\n",
      "Iteration 1624, loss = 0.42046606\n",
      "Iteration 1625, loss = 0.41987498\n",
      "Iteration 1626, loss = 0.41928493\n",
      "Iteration 1627, loss = 0.41869597\n",
      "Iteration 1628, loss = 0.41810833\n",
      "Iteration 1629, loss = 0.41752162\n",
      "Iteration 1630, loss = 0.41693598\n",
      "Iteration 1631, loss = 0.41635151\n",
      "Iteration 1632, loss = 0.41576816\n",
      "Iteration 1633, loss = 0.41518574\n",
      "Iteration 1634, loss = 0.41460446\n",
      "Iteration 1635, loss = 0.41402424\n",
      "Iteration 1636, loss = 0.41344514\n",
      "Iteration 1637, loss = 0.41286692\n",
      "Iteration 1638, loss = 0.41228992\n",
      "Iteration 1639, loss = 0.41171381\n",
      "Iteration 1640, loss = 0.41113890\n",
      "Iteration 1641, loss = 0.41056542\n",
      "Iteration 1642, loss = 0.40999282\n",
      "Iteration 1643, loss = 0.40942119\n",
      "Iteration 1644, loss = 0.40885056\n",
      "Iteration 1645, loss = 0.40828113\n",
      "Iteration 1646, loss = 0.40771264\n",
      "Iteration 1647, loss = 0.40714514\n",
      "Iteration 1648, loss = 0.40657861\n",
      "Iteration 1649, loss = 0.40601313\n",
      "Iteration 1650, loss = 0.40544858\n",
      "Iteration 1651, loss = 0.40488521\n",
      "Iteration 1652, loss = 0.40432263\n",
      "Iteration 1653, loss = 0.40376131\n",
      "Iteration 1654, loss = 0.40320080\n",
      "Iteration 1655, loss = 0.40264131\n",
      "Iteration 1656, loss = 0.40208297\n",
      "Iteration 1657, loss = 0.40152563\n",
      "Iteration 1658, loss = 0.40096919\n",
      "Iteration 1659, loss = 0.40041390\n",
      "Iteration 1660, loss = 0.39985950\n",
      "Iteration 1661, loss = 0.39930611\n",
      "Iteration 1662, loss = 0.39875381\n",
      "Iteration 1663, loss = 0.39820236\n",
      "Iteration 1664, loss = 0.39765207\n",
      "Iteration 1665, loss = 0.39710264\n",
      "Iteration 1666, loss = 0.39655428\n",
      "Iteration 1667, loss = 0.39600689\n",
      "Iteration 1668, loss = 0.39546044\n",
      "Iteration 1669, loss = 0.39491511\n",
      "Iteration 1670, loss = 0.39437071\n",
      "Iteration 1671, loss = 0.39382723\n",
      "Iteration 1672, loss = 0.39328499\n",
      "Iteration 1673, loss = 0.39274347\n",
      "Iteration 1674, loss = 0.39220317\n",
      "Iteration 1675, loss = 0.39166374\n",
      "Iteration 1676, loss = 0.39112534\n",
      "Iteration 1677, loss = 0.39058786\n",
      "Iteration 1678, loss = 0.39005130\n",
      "Iteration 1679, loss = 0.38951594\n",
      "Iteration 1680, loss = 0.38898124\n",
      "Iteration 1681, loss = 0.38844770\n",
      "Iteration 1682, loss = 0.38791504\n",
      "Iteration 1683, loss = 0.38738346\n",
      "Iteration 1684, loss = 0.38685280\n",
      "Iteration 1685, loss = 0.38632306\n",
      "Iteration 1686, loss = 0.38579430\n",
      "Iteration 1687, loss = 0.38526642\n",
      "Iteration 1688, loss = 0.38473948\n",
      "Iteration 1689, loss = 0.38421361\n",
      "Iteration 1690, loss = 0.38368848\n",
      "Iteration 1691, loss = 0.38316435\n",
      "Iteration 1692, loss = 0.38264131\n",
      "Iteration 1693, loss = 0.38211913\n",
      "Iteration 1694, loss = 0.38159792\n",
      "Iteration 1695, loss = 0.38107763\n",
      "Iteration 1696, loss = 0.38055826\n",
      "Iteration 1697, loss = 0.38003970\n",
      "Iteration 1698, loss = 0.37952211\n",
      "Iteration 1699, loss = 0.37900553\n",
      "Iteration 1700, loss = 0.37848979\n",
      "Iteration 1701, loss = 0.37797502\n",
      "Iteration 1702, loss = 0.37746110\n",
      "Iteration 1703, loss = 0.37694806\n",
      "Iteration 1704, loss = 0.37643599\n",
      "Iteration 1705, loss = 0.37592484\n",
      "Iteration 1706, loss = 0.37541477\n",
      "Iteration 1707, loss = 0.37490560\n",
      "Iteration 1708, loss = 0.37439730\n",
      "Iteration 1709, loss = 0.37388987\n",
      "Iteration 1710, loss = 0.37338336\n",
      "Iteration 1711, loss = 0.37287765\n",
      "Iteration 1712, loss = 0.37237285\n",
      "Iteration 1713, loss = 0.37186866\n",
      "Iteration 1714, loss = 0.37136538\n",
      "Iteration 1715, loss = 0.37086313\n",
      "Iteration 1716, loss = 0.37036165\n",
      "Iteration 1717, loss = 0.36986109\n",
      "Iteration 1718, loss = 0.36936131\n",
      "Iteration 1719, loss = 0.36886251\n",
      "Iteration 1720, loss = 0.36836462\n",
      "Iteration 1721, loss = 0.36786768\n",
      "Iteration 1722, loss = 0.36737139\n",
      "Iteration 1723, loss = 0.36687607\n",
      "Iteration 1724, loss = 0.36638164\n",
      "Iteration 1725, loss = 0.36588811\n",
      "Iteration 1726, loss = 0.36539543\n",
      "Iteration 1727, loss = 0.36490359\n",
      "Iteration 1728, loss = 0.36441265\n",
      "Iteration 1729, loss = 0.36392258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1730, loss = 0.36343336\n",
      "Iteration 1731, loss = 0.36294520\n",
      "Iteration 1732, loss = 0.36245769\n",
      "Iteration 1733, loss = 0.36197105\n",
      "Iteration 1734, loss = 0.36148525\n",
      "Iteration 1735, loss = 0.36100037\n",
      "Iteration 1736, loss = 0.36051634\n",
      "Iteration 1737, loss = 0.36003319\n",
      "Iteration 1738, loss = 0.35955090\n",
      "Iteration 1739, loss = 0.35906947\n",
      "Iteration 1740, loss = 0.35858881\n",
      "Iteration 1741, loss = 0.35810907\n",
      "Iteration 1742, loss = 0.35763020\n",
      "Iteration 1743, loss = 0.35715201\n",
      "Iteration 1744, loss = 0.35667485\n",
      "Iteration 1745, loss = 0.35619843\n",
      "Iteration 1746, loss = 0.35572286\n",
      "Iteration 1747, loss = 0.35524806\n",
      "Iteration 1748, loss = 0.35477424\n",
      "Iteration 1749, loss = 0.35430125\n",
      "Iteration 1750, loss = 0.35382926\n",
      "Iteration 1751, loss = 0.35335808\n",
      "Iteration 1752, loss = 0.35288767\n",
      "Iteration 1753, loss = 0.35241814\n",
      "Iteration 1754, loss = 0.35194954\n",
      "Iteration 1755, loss = 0.35148174\n",
      "Iteration 1756, loss = 0.35101483\n",
      "Iteration 1757, loss = 0.35054861\n",
      "Iteration 1758, loss = 0.35008341\n",
      "Iteration 1759, loss = 0.34961887\n",
      "Iteration 1760, loss = 0.34915514\n",
      "Iteration 1761, loss = 0.34869222\n",
      "Iteration 1762, loss = 0.34823017\n",
      "Iteration 1763, loss = 0.34776897\n",
      "Iteration 1764, loss = 0.34730847\n",
      "Iteration 1765, loss = 0.34684884\n",
      "Iteration 1766, loss = 0.34639012\n",
      "Iteration 1767, loss = 0.34593195\n",
      "Iteration 1768, loss = 0.34547483\n",
      "Iteration 1769, loss = 0.34501839\n",
      "Iteration 1770, loss = 0.34456272\n",
      "Iteration 1771, loss = 0.34410787\n",
      "Iteration 1772, loss = 0.34365386\n",
      "Iteration 1773, loss = 0.34320072\n",
      "Iteration 1774, loss = 0.34274840\n",
      "Iteration 1775, loss = 0.34229688\n",
      "Iteration 1776, loss = 0.34184616\n",
      "Iteration 1777, loss = 0.34139627\n",
      "Iteration 1778, loss = 0.34094712\n",
      "Iteration 1779, loss = 0.34049875\n",
      "Iteration 1780, loss = 0.34005120\n",
      "Iteration 1781, loss = 0.33960430\n",
      "Iteration 1782, loss = 0.33915833\n",
      "Iteration 1783, loss = 0.33871296\n",
      "Iteration 1784, loss = 0.33826842\n",
      "Iteration 1785, loss = 0.33782472\n",
      "Iteration 1786, loss = 0.33738181\n",
      "Iteration 1787, loss = 0.33693969\n",
      "Iteration 1788, loss = 0.33649814\n",
      "Iteration 1789, loss = 0.33605771\n",
      "Iteration 1790, loss = 0.33561781\n",
      "Iteration 1791, loss = 0.33517887\n",
      "Iteration 1792, loss = 0.33474070\n",
      "Iteration 1793, loss = 0.33430327\n",
      "Iteration 1794, loss = 0.33386670\n",
      "Iteration 1795, loss = 0.33343097\n",
      "Iteration 1796, loss = 0.33299598\n",
      "Iteration 1797, loss = 0.33256165\n",
      "Iteration 1798, loss = 0.33212822\n",
      "Iteration 1799, loss = 0.33169548\n",
      "Iteration 1800, loss = 0.33126355\n",
      "Iteration 1801, loss = 0.33083227\n",
      "Iteration 1802, loss = 0.33040188\n",
      "Iteration 1803, loss = 0.32997219\n",
      "Iteration 1804, loss = 0.32954323\n",
      "Iteration 1805, loss = 0.32911511\n",
      "Iteration 1806, loss = 0.32868764\n",
      "Iteration 1807, loss = 0.32826111\n",
      "Iteration 1808, loss = 0.32783511\n",
      "Iteration 1809, loss = 0.32741005\n",
      "Iteration 1810, loss = 0.32698577\n",
      "Iteration 1811, loss = 0.32656213\n",
      "Iteration 1812, loss = 0.32613935\n",
      "Iteration 1813, loss = 0.32571721\n",
      "Iteration 1814, loss = 0.32529588\n",
      "Iteration 1815, loss = 0.32487524\n",
      "Iteration 1816, loss = 0.32445531\n",
      "Iteration 1817, loss = 0.32403615\n",
      "Iteration 1818, loss = 0.32361761\n",
      "Iteration 1819, loss = 0.32319986\n",
      "Iteration 1820, loss = 0.32278282\n",
      "Iteration 1821, loss = 0.32236641\n",
      "Iteration 1822, loss = 0.32195083\n",
      "Iteration 1823, loss = 0.32153608\n",
      "Iteration 1824, loss = 0.32112190\n",
      "Iteration 1825, loss = 0.32070852\n",
      "Iteration 1826, loss = 0.32029582\n",
      "Iteration 1827, loss = 0.31988383\n",
      "Iteration 1828, loss = 0.31947255\n",
      "Iteration 1829, loss = 0.31906215\n",
      "Iteration 1830, loss = 0.31865246\n",
      "Iteration 1831, loss = 0.31824342\n",
      "Iteration 1832, loss = 0.31783507\n",
      "Iteration 1833, loss = 0.31742742\n",
      "Iteration 1834, loss = 0.31702050\n",
      "Iteration 1835, loss = 0.31661433\n",
      "Iteration 1836, loss = 0.31620872\n",
      "Iteration 1837, loss = 0.31580394\n",
      "Iteration 1838, loss = 0.31539985\n",
      "Iteration 1839, loss = 0.31499643\n",
      "Iteration 1840, loss = 0.31459381\n",
      "Iteration 1841, loss = 0.31419175\n",
      "Iteration 1842, loss = 0.31379051\n",
      "Iteration 1843, loss = 0.31338992\n",
      "Iteration 1844, loss = 0.31299005\n",
      "Iteration 1845, loss = 0.31259089\n",
      "Iteration 1846, loss = 0.31219243\n",
      "Iteration 1847, loss = 0.31179464\n",
      "Iteration 1848, loss = 0.31139753\n",
      "Iteration 1849, loss = 0.31100110\n",
      "Iteration 1850, loss = 0.31060537\n",
      "Iteration 1851, loss = 0.31021036\n",
      "Iteration 1852, loss = 0.30981603\n",
      "Iteration 1853, loss = 0.30942230\n",
      "Iteration 1854, loss = 0.30902938\n",
      "Iteration 1855, loss = 0.30863705\n",
      "Iteration 1856, loss = 0.30824542\n",
      "Iteration 1857, loss = 0.30785454\n",
      "Iteration 1858, loss = 0.30746423\n",
      "Iteration 1859, loss = 0.30707476\n",
      "Iteration 1860, loss = 0.30668577\n",
      "Iteration 1861, loss = 0.30629754\n",
      "Iteration 1862, loss = 0.30590993\n",
      "Iteration 1863, loss = 0.30552294\n",
      "Iteration 1864, loss = 0.30513697\n",
      "Iteration 1865, loss = 0.30475145\n",
      "Iteration 1866, loss = 0.30436665\n",
      "Iteration 1867, loss = 0.30398266\n",
      "Iteration 1868, loss = 0.30359912\n",
      "Iteration 1869, loss = 0.30321625\n",
      "Iteration 1870, loss = 0.30283412\n",
      "Iteration 1871, loss = 0.30245257\n",
      "Iteration 1872, loss = 0.30207176\n",
      "Iteration 1873, loss = 0.30169155\n",
      "Iteration 1874, loss = 0.30131202\n",
      "Iteration 1875, loss = 0.30093307\n",
      "Iteration 1876, loss = 0.30055483\n",
      "Iteration 1877, loss = 0.30017724\n",
      "Iteration 1878, loss = 0.29980031\n",
      "Iteration 1879, loss = 0.29942393\n",
      "Iteration 1880, loss = 0.29904830\n",
      "Iteration 1881, loss = 0.29867324\n",
      "Iteration 1882, loss = 0.29829886\n",
      "Iteration 1883, loss = 0.29792514\n",
      "Iteration 1884, loss = 0.29755210\n",
      "Iteration 1885, loss = 0.29717956\n",
      "Iteration 1886, loss = 0.29680780\n",
      "Iteration 1887, loss = 0.29643667\n",
      "Iteration 1888, loss = 0.29606615\n",
      "Iteration 1889, loss = 0.29569618\n",
      "Iteration 1890, loss = 0.29532692\n",
      "Iteration 1891, loss = 0.29495830\n",
      "Iteration 1892, loss = 0.29459031\n",
      "Iteration 1893, loss = 0.29422306\n",
      "Iteration 1894, loss = 0.29385640\n",
      "Iteration 1895, loss = 0.29349037\n",
      "Iteration 1896, loss = 0.29312494\n",
      "Iteration 1897, loss = 0.29276019\n",
      "Iteration 1898, loss = 0.29239598\n",
      "Iteration 1899, loss = 0.29203249\n",
      "Iteration 1900, loss = 0.29166948\n",
      "Iteration 1901, loss = 0.29130719\n",
      "Iteration 1902, loss = 0.29094551\n",
      "Iteration 1903, loss = 0.29058442\n",
      "Iteration 1904, loss = 0.29022400\n",
      "Iteration 1905, loss = 0.28986410\n",
      "Iteration 1906, loss = 0.28950482\n",
      "Iteration 1907, loss = 0.28914625\n",
      "Iteration 1908, loss = 0.28878820\n",
      "Iteration 1909, loss = 0.28843077\n",
      "Iteration 1910, loss = 0.28807401\n",
      "Iteration 1911, loss = 0.28771776\n",
      "Iteration 1912, loss = 0.28736222\n",
      "Iteration 1913, loss = 0.28700725\n",
      "Iteration 1914, loss = 0.28665289\n",
      "Iteration 1915, loss = 0.28629914\n",
      "Iteration 1916, loss = 0.28594597\n",
      "Iteration 1917, loss = 0.28559342\n",
      "Iteration 1918, loss = 0.28524143\n",
      "Iteration 1919, loss = 0.28489007\n",
      "Iteration 1920, loss = 0.28453933\n",
      "Iteration 1921, loss = 0.28418919\n",
      "Iteration 1922, loss = 0.28383978\n",
      "Iteration 1923, loss = 0.28349083\n",
      "Iteration 1924, loss = 0.28314255\n",
      "Iteration 1925, loss = 0.28279473\n",
      "Iteration 1926, loss = 0.28244759\n",
      "Iteration 1927, loss = 0.28210106\n",
      "Iteration 1928, loss = 0.28175504\n",
      "Iteration 1929, loss = 0.28140969\n",
      "Iteration 1930, loss = 0.28106488\n",
      "Iteration 1931, loss = 0.28072069\n",
      "Iteration 1932, loss = 0.28037708\n",
      "Iteration 1933, loss = 0.28003402\n",
      "Iteration 1934, loss = 0.27969150\n",
      "Iteration 1935, loss = 0.27934959\n",
      "Iteration 1936, loss = 0.27900824\n",
      "Iteration 1937, loss = 0.27866756\n",
      "Iteration 1938, loss = 0.27832722\n",
      "Iteration 1939, loss = 0.27798761\n",
      "Iteration 1940, loss = 0.27764845\n",
      "Iteration 1941, loss = 0.27730997\n",
      "Iteration 1942, loss = 0.27697195\n",
      "Iteration 1943, loss = 0.27663458\n",
      "Iteration 1944, loss = 0.27629792\n",
      "Iteration 1945, loss = 0.27596144\n",
      "Iteration 1946, loss = 0.27562589\n",
      "Iteration 1947, loss = 0.27529056\n",
      "Iteration 1948, loss = 0.27495592\n",
      "Iteration 1949, loss = 0.27462203\n",
      "Iteration 1950, loss = 0.27428837\n",
      "Iteration 1951, loss = 0.27395562\n",
      "Iteration 1952, loss = 0.27362315\n",
      "Iteration 1953, loss = 0.27329138\n",
      "Iteration 1954, loss = 0.27296014\n",
      "Iteration 1955, loss = 0.27262947\n",
      "Iteration 1956, loss = 0.27229950\n",
      "Iteration 1957, loss = 0.27197006\n",
      "Iteration 1958, loss = 0.27164115\n",
      "Iteration 1959, loss = 0.27131293\n",
      "Iteration 1960, loss = 0.27098513\n",
      "Iteration 1961, loss = 0.27065796\n",
      "Iteration 1962, loss = 0.27033131\n",
      "Iteration 1963, loss = 0.27000521\n",
      "Iteration 1964, loss = 0.26967957\n",
      "Iteration 1965, loss = 0.26935446\n",
      "Iteration 1966, loss = 0.26902984\n",
      "Iteration 1967, loss = 0.26870573\n",
      "Iteration 1968, loss = 0.26838216\n",
      "Iteration 1969, loss = 0.26805921\n",
      "Iteration 1970, loss = 0.26773667\n",
      "Iteration 1971, loss = 0.26741468\n",
      "Iteration 1972, loss = 0.26709320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1973, loss = 0.26677225\n",
      "Iteration 1974, loss = 0.26645181\n",
      "Iteration 1975, loss = 0.26613194\n",
      "Iteration 1976, loss = 0.26581254\n",
      "Iteration 1977, loss = 0.26549362\n",
      "Iteration 1978, loss = 0.26517529\n",
      "Iteration 1979, loss = 0.26485743\n",
      "Iteration 1980, loss = 0.26454006\n",
      "Iteration 1981, loss = 0.26422333\n",
      "Iteration 1982, loss = 0.26390693\n",
      "Iteration 1983, loss = 0.26359119\n",
      "Iteration 1984, loss = 0.26327593\n",
      "Iteration 1985, loss = 0.26296113\n",
      "Iteration 1986, loss = 0.26264689\n",
      "Iteration 1987, loss = 0.26233321\n",
      "Iteration 1988, loss = 0.26201989\n",
      "Iteration 1989, loss = 0.26170730\n",
      "Iteration 1990, loss = 0.26139506\n",
      "Iteration 1991, loss = 0.26108336\n",
      "Iteration 1992, loss = 0.26077222\n",
      "Iteration 1993, loss = 0.26046153\n",
      "Iteration 1994, loss = 0.26015136\n",
      "Iteration 1995, loss = 0.25984172\n",
      "Iteration 1996, loss = 0.25953260\n",
      "Iteration 1997, loss = 0.25922396\n",
      "Iteration 1998, loss = 0.25891593\n",
      "Iteration 1999, loss = 0.25860833\n",
      "Iteration 2000, loss = 0.25830128\n",
      "Iteration 2001, loss = 0.25799463\n",
      "Iteration 2002, loss = 0.25768855\n",
      "Iteration 2003, loss = 0.25738286\n",
      "Iteration 2004, loss = 0.25707783\n",
      "Iteration 2005, loss = 0.25677325\n",
      "Iteration 2006, loss = 0.25646901\n",
      "Iteration 2007, loss = 0.25616550\n",
      "Iteration 2008, loss = 0.25586234\n",
      "Iteration 2009, loss = 0.25555969\n",
      "Iteration 2010, loss = 0.25525758\n",
      "Iteration 2011, loss = 0.25495597\n",
      "Iteration 2012, loss = 0.25465499\n",
      "Iteration 2013, loss = 0.25435432\n",
      "Iteration 2014, loss = 0.25405424\n",
      "Iteration 2015, loss = 0.25375469\n",
      "Iteration 2016, loss = 0.25345556\n",
      "Iteration 2017, loss = 0.25315702\n",
      "Iteration 2018, loss = 0.25285891\n",
      "Iteration 2019, loss = 0.25256133\n",
      "Iteration 2020, loss = 0.25226435\n",
      "Iteration 2021, loss = 0.25196779\n",
      "Iteration 2022, loss = 0.25167180\n",
      "Iteration 2023, loss = 0.25137624\n",
      "Iteration 2024, loss = 0.25108117\n",
      "Iteration 2025, loss = 0.25078661\n",
      "Iteration 2026, loss = 0.25049250\n",
      "Iteration 2027, loss = 0.25019891\n",
      "Iteration 2028, loss = 0.24990581\n",
      "Iteration 2029, loss = 0.24961322\n",
      "Iteration 2030, loss = 0.24932113\n",
      "Iteration 2031, loss = 0.24902945\n",
      "Iteration 2032, loss = 0.24873838\n",
      "Iteration 2033, loss = 0.24844775\n",
      "Iteration 2034, loss = 0.24815758\n",
      "Iteration 2035, loss = 0.24786791\n",
      "Iteration 2036, loss = 0.24757871\n",
      "Iteration 2037, loss = 0.24729004\n",
      "Iteration 2038, loss = 0.24700180\n",
      "Iteration 2039, loss = 0.24671402\n",
      "Iteration 2040, loss = 0.24642678\n",
      "Iteration 2041, loss = 0.24613998\n",
      "Iteration 2042, loss = 0.24585367\n",
      "Iteration 2043, loss = 0.24556784\n",
      "Iteration 2044, loss = 0.24528239\n",
      "Iteration 2045, loss = 0.24499754\n",
      "Iteration 2046, loss = 0.24471306\n",
      "Iteration 2047, loss = 0.24442914\n",
      "Iteration 2048, loss = 0.24414561\n",
      "Iteration 2049, loss = 0.24386262\n",
      "Iteration 2050, loss = 0.24358004\n",
      "Iteration 2051, loss = 0.24329797\n",
      "Iteration 2052, loss = 0.24301635\n",
      "Iteration 2053, loss = 0.24273518\n",
      "Iteration 2054, loss = 0.24245448\n",
      "Iteration 2055, loss = 0.24217419\n",
      "Iteration 2056, loss = 0.24189431\n",
      "Iteration 2057, loss = 0.24161486\n",
      "Iteration 2058, loss = 0.24133598\n",
      "Iteration 2059, loss = 0.24105744\n",
      "Iteration 2060, loss = 0.24077930\n",
      "Iteration 2061, loss = 0.24050163\n",
      "Iteration 2062, loss = 0.24022441\n",
      "Iteration 2063, loss = 0.23994766\n",
      "Iteration 2064, loss = 0.23967134\n",
      "Iteration 2065, loss = 0.23939544\n",
      "Iteration 2066, loss = 0.23911999\n",
      "Iteration 2067, loss = 0.23884500\n",
      "Iteration 2068, loss = 0.23857042\n",
      "Iteration 2069, loss = 0.23829628\n",
      "Iteration 2070, loss = 0.23802269\n",
      "Iteration 2071, loss = 0.23774939\n",
      "Iteration 2072, loss = 0.23747660\n",
      "Iteration 2073, loss = 0.23720422\n",
      "Iteration 2074, loss = 0.23693229\n",
      "Iteration 2075, loss = 0.23666082\n",
      "Iteration 2076, loss = 0.23638974\n",
      "Iteration 2077, loss = 0.23611919\n",
      "Iteration 2078, loss = 0.23584899\n",
      "Iteration 2079, loss = 0.23557932\n",
      "Iteration 2080, loss = 0.23530999\n",
      "Iteration 2081, loss = 0.23504114\n",
      "Iteration 2082, loss = 0.23477269\n",
      "Iteration 2083, loss = 0.23450469\n",
      "Iteration 2084, loss = 0.23423711\n",
      "Iteration 2085, loss = 0.23396999\n",
      "Iteration 2086, loss = 0.23370334\n",
      "Iteration 2087, loss = 0.23343713\n",
      "Iteration 2088, loss = 0.23317121\n",
      "Iteration 2089, loss = 0.23290586\n",
      "Iteration 2090, loss = 0.23264086\n",
      "Iteration 2091, loss = 0.23237636\n",
      "Iteration 2092, loss = 0.23211229\n",
      "Iteration 2093, loss = 0.23184862\n",
      "Iteration 2094, loss = 0.23158543\n",
      "Iteration 2095, loss = 0.23132259\n",
      "Iteration 2096, loss = 0.23106019\n",
      "Iteration 2097, loss = 0.23079830\n",
      "Iteration 2098, loss = 0.23053671\n",
      "Iteration 2099, loss = 0.23027558\n",
      "Iteration 2100, loss = 0.23001487\n",
      "Iteration 2101, loss = 0.22975455\n",
      "Iteration 2102, loss = 0.22949457\n",
      "Iteration 2103, loss = 0.22923512\n",
      "Iteration 2104, loss = 0.22897606\n",
      "Iteration 2105, loss = 0.22871732\n",
      "Iteration 2106, loss = 0.22845904\n",
      "Iteration 2107, loss = 0.22820118\n",
      "Iteration 2108, loss = 0.22794376\n",
      "Iteration 2109, loss = 0.22768680\n",
      "Iteration 2110, loss = 0.22743018\n",
      "Iteration 2111, loss = 0.22717399\n",
      "Iteration 2112, loss = 0.22691822\n",
      "Iteration 2113, loss = 0.22666296\n",
      "Iteration 2114, loss = 0.22640804\n",
      "Iteration 2115, loss = 0.22615364\n",
      "Iteration 2116, loss = 0.22589957\n",
      "Iteration 2117, loss = 0.22564602\n",
      "Iteration 2118, loss = 0.22539274\n",
      "Iteration 2119, loss = 0.22513996\n",
      "Iteration 2120, loss = 0.22488750\n",
      "Iteration 2121, loss = 0.22463543\n",
      "Iteration 2122, loss = 0.22438381\n",
      "Iteration 2123, loss = 0.22413250\n",
      "Iteration 2124, loss = 0.22388166\n",
      "Iteration 2125, loss = 0.22363123\n",
      "Iteration 2126, loss = 0.22338121\n",
      "Iteration 2127, loss = 0.22313157\n",
      "Iteration 2128, loss = 0.22288231\n",
      "Iteration 2129, loss = 0.22263351\n",
      "Iteration 2130, loss = 0.22238511\n",
      "Iteration 2131, loss = 0.22213705\n",
      "Iteration 2132, loss = 0.22188932\n",
      "Iteration 2133, loss = 0.22164204\n",
      "Iteration 2134, loss = 0.22139506\n",
      "Iteration 2135, loss = 0.22114853\n",
      "Iteration 2136, loss = 0.22090233\n",
      "Iteration 2137, loss = 0.22065641\n",
      "Iteration 2138, loss = 0.22041091\n",
      "Iteration 2139, loss = 0.22016578\n",
      "Iteration 2140, loss = 0.21992108\n",
      "Iteration 2141, loss = 0.21967683\n",
      "Iteration 2142, loss = 0.21943299\n",
      "Iteration 2143, loss = 0.21918951\n",
      "Iteration 2144, loss = 0.21894641\n",
      "Iteration 2145, loss = 0.21870368\n",
      "Iteration 2146, loss = 0.21846136\n",
      "Iteration 2147, loss = 0.21821939\n",
      "Iteration 2148, loss = 0.21797785\n",
      "Iteration 2149, loss = 0.21773664\n",
      "Iteration 2150, loss = 0.21749581\n",
      "Iteration 2151, loss = 0.21725534\n",
      "Iteration 2152, loss = 0.21701534\n",
      "Iteration 2153, loss = 0.21677571\n",
      "Iteration 2154, loss = 0.21653645\n",
      "Iteration 2155, loss = 0.21629759\n",
      "Iteration 2156, loss = 0.21605911\n",
      "Iteration 2157, loss = 0.21582096\n",
      "Iteration 2158, loss = 0.21558317\n",
      "Iteration 2159, loss = 0.21534574\n",
      "Iteration 2160, loss = 0.21510873\n",
      "Iteration 2161, loss = 0.21487204\n",
      "Iteration 2162, loss = 0.21463570\n",
      "Iteration 2163, loss = 0.21439976\n",
      "Iteration 2164, loss = 0.21416425\n",
      "Iteration 2165, loss = 0.21392902\n",
      "Iteration 2166, loss = 0.21369421\n",
      "Iteration 2167, loss = 0.21345973\n",
      "Iteration 2168, loss = 0.21322565\n",
      "Iteration 2169, loss = 0.21299199\n",
      "Iteration 2170, loss = 0.21275870\n",
      "Iteration 2171, loss = 0.21252586\n",
      "Iteration 2172, loss = 0.21229322\n",
      "Iteration 2173, loss = 0.21206100\n",
      "Iteration 2174, loss = 0.21182912\n",
      "Iteration 2175, loss = 0.21159760\n",
      "Iteration 2176, loss = 0.21136663\n",
      "Iteration 2177, loss = 0.21113602\n",
      "Iteration 2178, loss = 0.21090586\n",
      "Iteration 2179, loss = 0.21067605\n",
      "Iteration 2180, loss = 0.21044661\n",
      "Iteration 2181, loss = 0.21021761\n",
      "Iteration 2182, loss = 0.20998884\n",
      "Iteration 2183, loss = 0.20976045\n",
      "Iteration 2184, loss = 0.20953247\n",
      "Iteration 2185, loss = 0.20930482\n",
      "Iteration 2186, loss = 0.20907761\n",
      "Iteration 2187, loss = 0.20885066\n",
      "Iteration 2188, loss = 0.20862413\n",
      "Iteration 2189, loss = 0.20839794\n",
      "Iteration 2190, loss = 0.20817211\n",
      "Iteration 2191, loss = 0.20794665\n",
      "Iteration 2192, loss = 0.20772153\n",
      "Iteration 2193, loss = 0.20749679\n",
      "Iteration 2194, loss = 0.20727237\n",
      "Iteration 2195, loss = 0.20704830\n",
      "Iteration 2196, loss = 0.20682461\n",
      "Iteration 2197, loss = 0.20660127\n",
      "Iteration 2198, loss = 0.20637838\n",
      "Iteration 2199, loss = 0.20615573\n",
      "Iteration 2200, loss = 0.20593350\n",
      "Iteration 2201, loss = 0.20571164\n",
      "Iteration 2202, loss = 0.20549010\n",
      "Iteration 2203, loss = 0.20526896\n",
      "Iteration 2204, loss = 0.20504812\n",
      "Iteration 2205, loss = 0.20482770\n",
      "Iteration 2206, loss = 0.20460755\n",
      "Iteration 2207, loss = 0.20438769\n",
      "Iteration 2208, loss = 0.20416829\n",
      "Iteration 2209, loss = 0.20394914\n",
      "Iteration 2210, loss = 0.20373034\n",
      "Iteration 2211, loss = 0.20351197\n",
      "Iteration 2212, loss = 0.20329384\n",
      "Iteration 2213, loss = 0.20307606\n",
      "Iteration 2214, loss = 0.20285868\n",
      "Iteration 2215, loss = 0.20264157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2216, loss = 0.20242483\n",
      "Iteration 2217, loss = 0.20220840\n",
      "Iteration 2218, loss = 0.20199236\n",
      "Iteration 2219, loss = 0.20177659\n",
      "Iteration 2220, loss = 0.20156120\n",
      "Iteration 2221, loss = 0.20134608\n",
      "Iteration 2222, loss = 0.20113135\n",
      "Iteration 2223, loss = 0.20091689\n",
      "Iteration 2224, loss = 0.20070280\n",
      "Iteration 2225, loss = 0.20048902\n",
      "Iteration 2226, loss = 0.20027551\n",
      "Iteration 2227, loss = 0.20006237\n",
      "Iteration 2228, loss = 0.19984957\n",
      "Iteration 2229, loss = 0.19963709\n",
      "Iteration 2230, loss = 0.19942490\n",
      "Iteration 2231, loss = 0.19921308\n",
      "Iteration 2232, loss = 0.19900156\n",
      "Iteration 2233, loss = 0.19879030\n",
      "Iteration 2234, loss = 0.19857941\n",
      "Iteration 2235, loss = 0.19836886\n",
      "Iteration 2236, loss = 0.19815856\n",
      "Iteration 2237, loss = 0.19794855\n",
      "Iteration 2238, loss = 0.19773885\n",
      "Iteration 2239, loss = 0.19752950\n",
      "Iteration 2240, loss = 0.19732049\n",
      "Iteration 2241, loss = 0.19711177\n",
      "Iteration 2242, loss = 0.19690342\n",
      "Iteration 2243, loss = 0.19669537\n",
      "Iteration 2244, loss = 0.19648766\n",
      "Iteration 2245, loss = 0.19628025\n",
      "Iteration 2246, loss = 0.19607322\n",
      "Iteration 2247, loss = 0.19586646\n",
      "Iteration 2248, loss = 0.19566005\n",
      "Iteration 2249, loss = 0.19545388\n",
      "Iteration 2250, loss = 0.19524808\n",
      "Iteration 2251, loss = 0.19504247\n",
      "Iteration 2252, loss = 0.19483730\n",
      "Iteration 2253, loss = 0.19463228\n",
      "Iteration 2254, loss = 0.19442764\n",
      "Iteration 2255, loss = 0.19422333\n",
      "Iteration 2256, loss = 0.19401923\n",
      "Iteration 2257, loss = 0.19381547\n",
      "Iteration 2258, loss = 0.19361201\n",
      "Iteration 2259, loss = 0.19340887\n",
      "Iteration 2260, loss = 0.19320602\n",
      "Iteration 2261, loss = 0.19300352\n",
      "Iteration 2262, loss = 0.19280120\n",
      "Iteration 2263, loss = 0.19259924\n",
      "Iteration 2264, loss = 0.19239757\n",
      "Iteration 2265, loss = 0.19219617\n",
      "Iteration 2266, loss = 0.19199511\n",
      "Iteration 2267, loss = 0.19179436\n",
      "Iteration 2268, loss = 0.19159396\n",
      "Iteration 2269, loss = 0.19139381\n",
      "Iteration 2270, loss = 0.19119401\n",
      "Iteration 2271, loss = 0.19099449\n",
      "Iteration 2272, loss = 0.19079528\n",
      "Iteration 2273, loss = 0.19059640\n",
      "Iteration 2274, loss = 0.19039770\n",
      "Iteration 2275, loss = 0.19019935\n",
      "Iteration 2276, loss = 0.19000131\n",
      "Iteration 2277, loss = 0.18980357\n",
      "Iteration 2278, loss = 0.18960620\n",
      "Iteration 2279, loss = 0.18940906\n",
      "Iteration 2280, loss = 0.18921224\n",
      "Iteration 2281, loss = 0.18901579\n",
      "Iteration 2282, loss = 0.18881957\n",
      "Iteration 2283, loss = 0.18862374\n",
      "Iteration 2284, loss = 0.18842821\n",
      "Iteration 2285, loss = 0.18823289\n",
      "Iteration 2286, loss = 0.18803803\n",
      "Iteration 2287, loss = 0.18784337\n",
      "Iteration 2288, loss = 0.18764901\n",
      "Iteration 2289, loss = 0.18745510\n",
      "Iteration 2290, loss = 0.18726146\n",
      "Iteration 2291, loss = 0.18706805\n",
      "Iteration 2292, loss = 0.18687502\n",
      "Iteration 2293, loss = 0.18668229\n",
      "Iteration 2294, loss = 0.18648985\n",
      "Iteration 2295, loss = 0.18629777\n",
      "Iteration 2296, loss = 0.18610598\n",
      "Iteration 2297, loss = 0.18591454\n",
      "Iteration 2298, loss = 0.18572333\n",
      "Iteration 2299, loss = 0.18553249\n",
      "Iteration 2300, loss = 0.18534189\n",
      "Iteration 2301, loss = 0.18515164\n",
      "Iteration 2302, loss = 0.18496162\n",
      "Iteration 2303, loss = 0.18477191\n",
      "Iteration 2304, loss = 0.18458247\n",
      "Iteration 2305, loss = 0.18439336\n",
      "Iteration 2306, loss = 0.18420445\n",
      "Iteration 2307, loss = 0.18401588\n",
      "Iteration 2308, loss = 0.18382762\n",
      "Iteration 2309, loss = 0.18363962\n",
      "Iteration 2310, loss = 0.18345195\n",
      "Iteration 2311, loss = 0.18326454\n",
      "Iteration 2312, loss = 0.18307739\n",
      "Iteration 2313, loss = 0.18289056\n",
      "Iteration 2314, loss = 0.18270401\n",
      "Iteration 2315, loss = 0.18251775\n",
      "Iteration 2316, loss = 0.18233176\n",
      "Iteration 2317, loss = 0.18214610\n",
      "Iteration 2318, loss = 0.18196062\n",
      "Iteration 2319, loss = 0.18177552\n",
      "Iteration 2320, loss = 0.18159069\n",
      "Iteration 2321, loss = 0.18140610\n",
      "Iteration 2322, loss = 0.18122183\n",
      "Iteration 2323, loss = 0.18103780\n",
      "Iteration 2324, loss = 0.18085406\n",
      "Iteration 2325, loss = 0.18067059\n",
      "Iteration 2326, loss = 0.18048743\n",
      "Iteration 2327, loss = 0.18030451\n",
      "Iteration 2328, loss = 0.18012189\n",
      "Iteration 2329, loss = 0.17993958\n",
      "Iteration 2330, loss = 0.17975758\n",
      "Iteration 2331, loss = 0.17957591\n",
      "Iteration 2332, loss = 0.17939460\n",
      "Iteration 2333, loss = 0.17921355\n",
      "Iteration 2334, loss = 0.17903279\n",
      "Iteration 2335, loss = 0.17885234\n",
      "Iteration 2336, loss = 0.17867214\n",
      "Iteration 2337, loss = 0.17849226\n",
      "Iteration 2338, loss = 0.17831263\n",
      "Iteration 2339, loss = 0.17813332\n",
      "Iteration 2340, loss = 0.17795422\n",
      "Iteration 2341, loss = 0.17777546\n",
      "Iteration 2342, loss = 0.17759695\n",
      "Iteration 2343, loss = 0.17741877\n",
      "Iteration 2344, loss = 0.17724079\n",
      "Iteration 2345, loss = 0.17706311\n",
      "Iteration 2346, loss = 0.17688570\n",
      "Iteration 2347, loss = 0.17670856\n",
      "Iteration 2348, loss = 0.17653168\n",
      "Iteration 2349, loss = 0.17635510\n",
      "Iteration 2350, loss = 0.17617876\n",
      "Iteration 2351, loss = 0.17600269\n",
      "Iteration 2352, loss = 0.17582690\n",
      "Iteration 2353, loss = 0.17565134\n",
      "Iteration 2354, loss = 0.17547612\n",
      "Iteration 2355, loss = 0.17530116\n",
      "Iteration 2356, loss = 0.17512649\n",
      "Iteration 2357, loss = 0.17495214\n",
      "Iteration 2358, loss = 0.17477807\n",
      "Iteration 2359, loss = 0.17460429\n",
      "Iteration 2360, loss = 0.17443072\n",
      "Iteration 2361, loss = 0.17425744\n",
      "Iteration 2362, loss = 0.17408445\n",
      "Iteration 2363, loss = 0.17391173\n",
      "Iteration 2364, loss = 0.17373924\n",
      "Iteration 2365, loss = 0.17356701\n",
      "Iteration 2366, loss = 0.17339507\n",
      "Iteration 2367, loss = 0.17322339\n",
      "Iteration 2368, loss = 0.17305197\n",
      "Iteration 2369, loss = 0.17288079\n",
      "Iteration 2370, loss = 0.17270983\n",
      "Iteration 2371, loss = 0.17253919\n",
      "Iteration 2372, loss = 0.17236880\n",
      "Iteration 2373, loss = 0.17219865\n",
      "Iteration 2374, loss = 0.17202875\n",
      "Iteration 2375, loss = 0.17185909\n",
      "Iteration 2376, loss = 0.17168969\n",
      "Iteration 2377, loss = 0.17152052\n",
      "Iteration 2378, loss = 0.17135164\n",
      "Iteration 2379, loss = 0.17118298\n",
      "Iteration 2380, loss = 0.17101461\n",
      "Iteration 2381, loss = 0.17084654\n",
      "Iteration 2382, loss = 0.17067869\n",
      "Iteration 2383, loss = 0.17051106\n",
      "Iteration 2384, loss = 0.17034372\n",
      "Iteration 2385, loss = 0.17017665\n",
      "Iteration 2386, loss = 0.17000979\n",
      "Iteration 2387, loss = 0.16984320\n",
      "Iteration 2388, loss = 0.16967685\n",
      "Iteration 2389, loss = 0.16951080\n",
      "Iteration 2390, loss = 0.16934490\n",
      "Iteration 2391, loss = 0.16917929\n",
      "Iteration 2392, loss = 0.16901394\n",
      "Iteration 2393, loss = 0.16884885\n",
      "Iteration 2394, loss = 0.16868395\n",
      "Iteration 2395, loss = 0.16851936\n",
      "Iteration 2396, loss = 0.16835496\n",
      "Iteration 2397, loss = 0.16819083\n",
      "Iteration 2398, loss = 0.16802697\n",
      "Iteration 2399, loss = 0.16786330\n",
      "Iteration 2400, loss = 0.16769991\n",
      "Iteration 2401, loss = 0.16753677\n",
      "Iteration 2402, loss = 0.16737386\n",
      "Iteration 2403, loss = 0.16721119\n",
      "Iteration 2404, loss = 0.16704877\n",
      "Iteration 2405, loss = 0.16688656\n",
      "Iteration 2406, loss = 0.16672465\n",
      "Iteration 2407, loss = 0.16656296\n",
      "Iteration 2408, loss = 0.16640152\n",
      "Iteration 2409, loss = 0.16624027\n",
      "Iteration 2410, loss = 0.16607930\n",
      "Iteration 2411, loss = 0.16591855\n",
      "Iteration 2412, loss = 0.16575808\n",
      "Iteration 2413, loss = 0.16559783\n",
      "Iteration 2414, loss = 0.16543778\n",
      "Iteration 2415, loss = 0.16527796\n",
      "Iteration 2416, loss = 0.16511837\n",
      "Iteration 2417, loss = 0.16495908\n",
      "Iteration 2418, loss = 0.16480004\n",
      "Iteration 2419, loss = 0.16464116\n",
      "Iteration 2420, loss = 0.16448258\n",
      "Iteration 2421, loss = 0.16432425\n",
      "Iteration 2422, loss = 0.16416610\n",
      "Iteration 2423, loss = 0.16400819\n",
      "Iteration 2424, loss = 0.16385054\n",
      "Iteration 2425, loss = 0.16369311\n",
      "Iteration 2426, loss = 0.16353590\n",
      "Iteration 2427, loss = 0.16337890\n",
      "Iteration 2428, loss = 0.16322215\n",
      "Iteration 2429, loss = 0.16306553\n",
      "Iteration 2430, loss = 0.16290916\n",
      "Iteration 2431, loss = 0.16275301\n",
      "Iteration 2432, loss = 0.16259708\n",
      "Iteration 2433, loss = 0.16244131\n",
      "Iteration 2434, loss = 0.16228584\n",
      "Iteration 2435, loss = 0.16213056\n",
      "Iteration 2436, loss = 0.16197547\n",
      "Iteration 2437, loss = 0.16182062\n",
      "Iteration 2438, loss = 0.16166599\n",
      "Iteration 2439, loss = 0.16151159\n",
      "Iteration 2440, loss = 0.16135739\n",
      "Iteration 2441, loss = 0.16120343\n",
      "Iteration 2442, loss = 0.16104971\n",
      "Iteration 2443, loss = 0.16089622\n",
      "Iteration 2444, loss = 0.16074299\n",
      "Iteration 2445, loss = 0.16059002\n",
      "Iteration 2446, loss = 0.16043723\n",
      "Iteration 2447, loss = 0.16028468\n",
      "Iteration 2448, loss = 0.16013239\n",
      "Iteration 2449, loss = 0.15998031\n",
      "Iteration 2450, loss = 0.15982843\n",
      "Iteration 2451, loss = 0.15967677\n",
      "Iteration 2452, loss = 0.15952541\n",
      "Iteration 2453, loss = 0.15937423\n",
      "Iteration 2454, loss = 0.15922324\n",
      "Iteration 2455, loss = 0.15907253\n",
      "Iteration 2456, loss = 0.15892205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2457, loss = 0.15877179\n",
      "Iteration 2458, loss = 0.15862175\n",
      "Iteration 2459, loss = 0.15847201\n",
      "Iteration 2460, loss = 0.15832236\n",
      "Iteration 2461, loss = 0.15817299\n",
      "Iteration 2462, loss = 0.15802380\n",
      "Iteration 2463, loss = 0.15787484\n",
      "Iteration 2464, loss = 0.15772610\n",
      "Iteration 2465, loss = 0.15757756\n",
      "Iteration 2466, loss = 0.15742926\n",
      "Iteration 2467, loss = 0.15728119\n",
      "Iteration 2468, loss = 0.15713333\n",
      "Iteration 2469, loss = 0.15698572\n",
      "Iteration 2470, loss = 0.15683829\n",
      "Iteration 2471, loss = 0.15669114\n",
      "Iteration 2472, loss = 0.15654418\n",
      "Iteration 2473, loss = 0.15639745\n",
      "Iteration 2474, loss = 0.15625093\n",
      "Iteration 2475, loss = 0.15610460\n",
      "Iteration 2476, loss = 0.15595852\n",
      "Iteration 2477, loss = 0.15581268\n",
      "Iteration 2478, loss = 0.15566706\n",
      "Iteration 2479, loss = 0.15552155\n",
      "Iteration 2480, loss = 0.15537635\n",
      "Iteration 2481, loss = 0.15523134\n",
      "Iteration 2482, loss = 0.15508654\n",
      "Iteration 2483, loss = 0.15494196\n",
      "Iteration 2484, loss = 0.15479757\n",
      "Iteration 2485, loss = 0.15465337\n",
      "Iteration 2486, loss = 0.15450942\n",
      "Iteration 2487, loss = 0.15436569\n",
      "Iteration 2488, loss = 0.15422212\n",
      "Iteration 2489, loss = 0.15407878\n",
      "Iteration 2490, loss = 0.15393565\n",
      "Iteration 2491, loss = 0.15379269\n",
      "Iteration 2492, loss = 0.15364998\n",
      "Iteration 2493, loss = 0.15350745\n",
      "Iteration 2494, loss = 0.15336511\n",
      "Iteration 2495, loss = 0.15322300\n",
      "Iteration 2496, loss = 0.15308104\n",
      "Iteration 2497, loss = 0.15293937\n",
      "Iteration 2498, loss = 0.15279785\n",
      "Iteration 2499, loss = 0.15265652\n",
      "Iteration 2500, loss = 0.15251544\n",
      "Iteration 2501, loss = 0.15237460\n",
      "Iteration 2502, loss = 0.15223390\n",
      "Iteration 2503, loss = 0.15209343\n",
      "Iteration 2504, loss = 0.15195318\n",
      "Iteration 2505, loss = 0.15181308\n",
      "Iteration 2506, loss = 0.15167320\n",
      "Iteration 2507, loss = 0.15153355\n",
      "Iteration 2508, loss = 0.15139404\n",
      "Iteration 2509, loss = 0.15125478\n",
      "Iteration 2510, loss = 0.15111569\n",
      "Iteration 2511, loss = 0.15097685\n",
      "Iteration 2512, loss = 0.15083815\n",
      "Iteration 2513, loss = 0.15069967\n",
      "Iteration 2514, loss = 0.15056138\n",
      "Iteration 2515, loss = 0.15042333\n",
      "Iteration 2516, loss = 0.15028546\n",
      "Iteration 2517, loss = 0.15014778\n",
      "Iteration 2518, loss = 0.15001030\n",
      "Iteration 2519, loss = 0.14987300\n",
      "Iteration 2520, loss = 0.14973595\n",
      "Iteration 2521, loss = 0.14959904\n",
      "Iteration 2522, loss = 0.14946235\n",
      "Iteration 2523, loss = 0.14932582\n",
      "Iteration 2524, loss = 0.14918952\n",
      "Iteration 2525, loss = 0.14905340\n",
      "Iteration 2526, loss = 0.14891753\n",
      "Iteration 2527, loss = 0.14878184\n",
      "Iteration 2528, loss = 0.14864632\n",
      "Iteration 2529, loss = 0.14851103\n",
      "Iteration 2530, loss = 0.14837591\n",
      "Iteration 2531, loss = 0.14824100\n",
      "Iteration 2532, loss = 0.14810623\n",
      "Iteration 2533, loss = 0.14797171\n",
      "Iteration 2534, loss = 0.14783737\n",
      "Iteration 2535, loss = 0.14770317\n",
      "Iteration 2536, loss = 0.14756918\n",
      "Iteration 2537, loss = 0.14743540\n",
      "Iteration 2538, loss = 0.14730177\n",
      "Iteration 2539, loss = 0.14716836\n",
      "Iteration 2540, loss = 0.14703514\n",
      "Iteration 2541, loss = 0.14690210\n",
      "Iteration 2542, loss = 0.14676925\n",
      "Iteration 2543, loss = 0.14663658\n",
      "Iteration 2544, loss = 0.14650406\n",
      "Iteration 2545, loss = 0.14637178\n",
      "Iteration 2546, loss = 0.14623967\n",
      "Iteration 2547, loss = 0.14610773\n",
      "Iteration 2548, loss = 0.14597599\n",
      "Iteration 2549, loss = 0.14584443\n",
      "Iteration 2550, loss = 0.14571305\n",
      "Iteration 2551, loss = 0.14558187\n",
      "Iteration 2552, loss = 0.14545090\n",
      "Iteration 2553, loss = 0.14532006\n",
      "Iteration 2554, loss = 0.14518942\n",
      "Iteration 2555, loss = 0.14505900\n",
      "Iteration 2556, loss = 0.14492873\n",
      "Iteration 2557, loss = 0.14479866\n",
      "Iteration 2558, loss = 0.14466875\n",
      "Iteration 2559, loss = 0.14453903\n",
      "Iteration 2560, loss = 0.14440950\n",
      "Iteration 2561, loss = 0.14428018\n",
      "Iteration 2562, loss = 0.14415098\n",
      "Iteration 2563, loss = 0.14402201\n",
      "Iteration 2564, loss = 0.14389318\n",
      "Iteration 2565, loss = 0.14376452\n",
      "Iteration 2566, loss = 0.14363608\n",
      "Iteration 2567, loss = 0.14350781\n",
      "Iteration 2568, loss = 0.14337967\n",
      "Iteration 2569, loss = 0.14325173\n",
      "Iteration 2570, loss = 0.14312398\n",
      "Iteration 2571, loss = 0.14299644\n",
      "Iteration 2572, loss = 0.14286908\n",
      "Iteration 2573, loss = 0.14274187\n",
      "Iteration 2574, loss = 0.14261485\n",
      "Iteration 2575, loss = 0.14248802\n",
      "Iteration 2576, loss = 0.14236134\n",
      "Iteration 2577, loss = 0.14223489\n",
      "Iteration 2578, loss = 0.14210855\n",
      "Iteration 2579, loss = 0.14198245\n",
      "Iteration 2580, loss = 0.14185645\n",
      "Iteration 2581, loss = 0.14173072\n",
      "Iteration 2582, loss = 0.14160507\n",
      "Iteration 2583, loss = 0.14147968\n",
      "Iteration 2584, loss = 0.14135440\n",
      "Iteration 2585, loss = 0.14122929\n",
      "Iteration 2586, loss = 0.14110440\n",
      "Iteration 2587, loss = 0.14097966\n",
      "Iteration 2588, loss = 0.14085509\n",
      "Iteration 2589, loss = 0.14073071\n",
      "Iteration 2590, loss = 0.14060651\n",
      "Iteration 2591, loss = 0.14048248\n",
      "Iteration 2592, loss = 0.14035862\n",
      "Iteration 2593, loss = 0.14023492\n",
      "Iteration 2594, loss = 0.14011139\n",
      "Iteration 2595, loss = 0.13998807\n",
      "Iteration 2596, loss = 0.13986492\n",
      "Iteration 2597, loss = 0.13974193\n",
      "Iteration 2598, loss = 0.13961912\n",
      "Iteration 2599, loss = 0.13949650\n",
      "Iteration 2600, loss = 0.13937406\n",
      "Iteration 2601, loss = 0.13925177\n",
      "Iteration 2602, loss = 0.13912962\n",
      "Iteration 2603, loss = 0.13900772\n",
      "Iteration 2604, loss = 0.13888591\n",
      "Iteration 2605, loss = 0.13876431\n",
      "Iteration 2606, loss = 0.13864287\n",
      "Iteration 2607, loss = 0.13852158\n",
      "Iteration 2608, loss = 0.13840048\n",
      "Iteration 2609, loss = 0.13827955\n",
      "Iteration 2610, loss = 0.13815877\n",
      "Iteration 2611, loss = 0.13803820\n",
      "Iteration 2612, loss = 0.13791775\n",
      "Iteration 2613, loss = 0.13779750\n",
      "Iteration 2614, loss = 0.13767740\n",
      "Iteration 2615, loss = 0.13755750\n",
      "Iteration 2616, loss = 0.13743776\n",
      "Iteration 2617, loss = 0.13731817\n",
      "Iteration 2618, loss = 0.13719875\n",
      "Iteration 2619, loss = 0.13707949\n",
      "Iteration 2620, loss = 0.13696042\n",
      "Iteration 2621, loss = 0.13684146\n",
      "Iteration 2622, loss = 0.13672270\n",
      "Iteration 2623, loss = 0.13660408\n",
      "Iteration 2624, loss = 0.13648565\n",
      "Iteration 2625, loss = 0.13636738\n",
      "Iteration 2626, loss = 0.13624930\n",
      "Iteration 2627, loss = 0.13613135\n",
      "Iteration 2628, loss = 0.13601359\n",
      "Iteration 2629, loss = 0.13589597\n",
      "Iteration 2630, loss = 0.13577851\n",
      "Iteration 2631, loss = 0.13566125\n",
      "Iteration 2632, loss = 0.13554413\n",
      "Iteration 2633, loss = 0.13542716\n",
      "Iteration 2634, loss = 0.13531039\n",
      "Iteration 2635, loss = 0.13519377\n",
      "Iteration 2636, loss = 0.13507728\n",
      "Iteration 2637, loss = 0.13496098\n",
      "Iteration 2638, loss = 0.13484482\n",
      "Iteration 2639, loss = 0.13472885\n",
      "Iteration 2640, loss = 0.13461302\n",
      "Iteration 2641, loss = 0.13449736\n",
      "Iteration 2642, loss = 0.13438188\n",
      "Iteration 2643, loss = 0.13426653\n",
      "Iteration 2644, loss = 0.13415132\n",
      "Iteration 2645, loss = 0.13403630\n",
      "Iteration 2646, loss = 0.13392140\n",
      "Iteration 2647, loss = 0.13380666\n",
      "Iteration 2648, loss = 0.13369209\n",
      "Iteration 2649, loss = 0.13357765\n",
      "Iteration 2650, loss = 0.13346337\n",
      "Iteration 2651, loss = 0.13334927\n",
      "Iteration 2652, loss = 0.13323529\n",
      "Iteration 2653, loss = 0.13312147\n",
      "Iteration 2654, loss = 0.13300783\n",
      "Iteration 2655, loss = 0.13289434\n",
      "Iteration 2656, loss = 0.13278098\n",
      "Iteration 2657, loss = 0.13266777\n",
      "Iteration 2658, loss = 0.13255473\n",
      "Iteration 2659, loss = 0.13244185\n",
      "Iteration 2660, loss = 0.13232910\n",
      "Iteration 2661, loss = 0.13221652\n",
      "Iteration 2662, loss = 0.13210410\n",
      "Iteration 2663, loss = 0.13199180\n",
      "Iteration 2664, loss = 0.13187969\n",
      "Iteration 2665, loss = 0.13176766\n",
      "Iteration 2666, loss = 0.13165587\n",
      "Iteration 2667, loss = 0.13154419\n",
      "Iteration 2668, loss = 0.13143266\n",
      "Iteration 2669, loss = 0.13132129\n",
      "Iteration 2670, loss = 0.13121004\n",
      "Iteration 2671, loss = 0.13109895\n",
      "Iteration 2672, loss = 0.13098805\n",
      "Iteration 2673, loss = 0.13087722\n",
      "Iteration 2674, loss = 0.13076660\n",
      "Iteration 2675, loss = 0.13065612\n",
      "Iteration 2676, loss = 0.13054577\n",
      "Iteration 2677, loss = 0.13043556\n",
      "Iteration 2678, loss = 0.13032555\n",
      "Iteration 2679, loss = 0.13021565\n",
      "Iteration 2680, loss = 0.13010592\n",
      "Iteration 2681, loss = 0.12999633\n",
      "Iteration 2682, loss = 0.12988690\n",
      "Iteration 2683, loss = 0.12977759\n",
      "Iteration 2684, loss = 0.12966843\n",
      "Iteration 2685, loss = 0.12955944\n",
      "Iteration 2686, loss = 0.12945060\n",
      "Iteration 2687, loss = 0.12934193\n",
      "Iteration 2688, loss = 0.12923337\n",
      "Iteration 2689, loss = 0.12912500\n",
      "Iteration 2690, loss = 0.12901674\n",
      "Iteration 2691, loss = 0.12890863\n",
      "Iteration 2692, loss = 0.12880066\n",
      "Iteration 2693, loss = 0.12869287\n",
      "Iteration 2694, loss = 0.12858521\n",
      "Iteration 2695, loss = 0.12847769\n",
      "Iteration 2696, loss = 0.12837030\n",
      "Iteration 2697, loss = 0.12826308\n",
      "Iteration 2698, loss = 0.12815598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2699, loss = 0.12804903\n",
      "Iteration 2700, loss = 0.12794225\n",
      "Iteration 2701, loss = 0.12783555\n",
      "Iteration 2702, loss = 0.12772905\n",
      "Iteration 2703, loss = 0.12762269\n",
      "Iteration 2704, loss = 0.12751644\n",
      "Iteration 2705, loss = 0.12741036\n",
      "Iteration 2706, loss = 0.12730442\n",
      "Iteration 2707, loss = 0.12719861\n",
      "Iteration 2708, loss = 0.12709295\n",
      "Iteration 2709, loss = 0.12698744\n",
      "Iteration 2710, loss = 0.12688206\n",
      "Iteration 2711, loss = 0.12677683\n",
      "Iteration 2712, loss = 0.12667172\n",
      "Iteration 2713, loss = 0.12656678\n",
      "Iteration 2714, loss = 0.12646196\n",
      "Iteration 2715, loss = 0.12635729\n",
      "Iteration 2716, loss = 0.12625274\n",
      "Iteration 2717, loss = 0.12614833\n",
      "Iteration 2718, loss = 0.12604407\n",
      "Iteration 2719, loss = 0.12593993\n",
      "Iteration 2720, loss = 0.12583594\n",
      "Iteration 2721, loss = 0.12573209\n",
      "Iteration 2722, loss = 0.12562837\n",
      "Iteration 2723, loss = 0.12552478\n",
      "Iteration 2724, loss = 0.12542134\n",
      "Iteration 2725, loss = 0.12531802\n",
      "Iteration 2726, loss = 0.12521484\n",
      "Iteration 2727, loss = 0.12511178\n",
      "Iteration 2728, loss = 0.12500887\n",
      "Iteration 2729, loss = 0.12490609\n",
      "Iteration 2730, loss = 0.12480348\n",
      "Iteration 2731, loss = 0.12470096\n",
      "Iteration 2732, loss = 0.12459861\n",
      "Iteration 2733, loss = 0.12449638\n",
      "Iteration 2734, loss = 0.12439429\n",
      "Iteration 2735, loss = 0.12429233\n",
      "Iteration 2736, loss = 0.12419056\n",
      "Iteration 2737, loss = 0.12408893\n",
      "Iteration 2738, loss = 0.12398743\n",
      "Iteration 2739, loss = 0.12388605\n",
      "Iteration 2740, loss = 0.12378483\n",
      "Iteration 2741, loss = 0.12368374\n",
      "Iteration 2742, loss = 0.12358280\n",
      "Iteration 2743, loss = 0.12348200\n",
      "Iteration 2744, loss = 0.12338131\n",
      "Iteration 2745, loss = 0.12328079\n",
      "Iteration 2746, loss = 0.12318038\n",
      "Iteration 2747, loss = 0.12308012\n",
      "Iteration 2748, loss = 0.12297998\n",
      "Iteration 2749, loss = 0.12287996\n",
      "Iteration 2750, loss = 0.12278010\n",
      "Iteration 2751, loss = 0.12268033\n",
      "Iteration 2752, loss = 0.12258070\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "\n",
      "Training finished.\n",
      "\n",
      "[NN] Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "path = 'C:\\\\Users\\\\Sherif\\\\Desktop\\\\IP_Project\\\\Notes'\n",
    "testPaths = 'C:\\\\Users\\\\Sherif\\\\Desktop\\\\IP_Project\\\\Test'\n",
    "t_size = 0.01  # test_Size\n",
    "r_state = 42   # Random Seed\n",
    "sizeAfterResize = (32, 32)  # Size of images after resize\n",
    "jobs = 1\n",
    "kNeighbours = 7\n",
    "imagePaths = list(paths.list_images(path))\n",
    "testImagePaths = list(paths.list_images(testPaths))\n",
    "features = []\n",
    "labels = []\n",
    "testImages = list()\n",
    "testLabels = []\n",
    "\n",
    "\n",
    "def extractHog(image):\n",
    "    image = cv2.resize(image, sizeAfterResize)\n",
    "    winSize = (32, 32)  # Image Size\n",
    "    cellSize = (4, 4)  # Size of one cell\n",
    "    blockSizeInCells = (2, 2)  # will be multiplies by No of cells\n",
    "\n",
    "    blockSize = (blockSizeInCells[1] * cellSize[1], blockSizeInCells[0] * cellSize[0])\n",
    "    blockStride = (cellSize[1], cellSize[0])\n",
    "    nbins = 12  # Number of orientation bins\n",
    "    hog = cv2.HOGDescriptor(winSize, blockSize, blockStride, cellSize, nbins)\n",
    "    h = hog.compute(image)\n",
    "    h = h.flatten()\n",
    "    return h.flatten()\n",
    "\n",
    "\n",
    "for (i, imagePath) in enumerate(imagePaths):\n",
    "    # Load image and labels\n",
    "    # path format: path/{class}.{image_num}.jpg\n",
    "    image = cv2.imread(imagePath)\n",
    "\n",
    "    label = imagePath.split(os.path.sep)[-1].split(\".\")[0]\n",
    "\n",
    "    sampleFeatures = extractHog(image)\n",
    "\n",
    "    features.append(sampleFeatures)\n",
    "\n",
    "    labels.append(label)\n",
    "\n",
    "\n",
    "for (j, imagePath) in enumerate(testImagePaths):\n",
    "    image = cv2.imread(imagePath)\n",
    "\n",
    "    testLabel = imagePath.split(os.path.sep)[-1].split(\".\")[0]\n",
    "\n",
    "    imageFeatures = extractHog(image)\n",
    "\n",
    "    testImages.append(imageFeatures)\n",
    "\n",
    "    testLabels.append(testLabel)\n",
    "\n",
    "\n",
    "features = np.array(features)\n",
    "\n",
    "labels = np.array(labels)\n",
    "\n",
    "# (trainFeatures, testImages1, trainLabels, testLabels1) = train_test_split(\n",
    "#     features, labels, test_size=t_size, random_state=r_state)\n",
    "\n",
    "NN_model = MLPClassifier(hidden_layer_sizes=(500, ), solver='sgd', alpha=1e-5, random_state=100, max_iter=4000, verbose=1)\n",
    "\n",
    "NN_model.fit(features, labels)\n",
    "\n",
    "print('\\nTraining finished.')\n",
    "\n",
    "acc = NN_model.score(testImages, testLabels)\n",
    "\n",
    "print(\"\\n[NN] Accuracy: {:.2f}%\".format(acc * 100))\n",
    "\n",
    "# joblib.dump(NN_model, 'D:\\\\IP_Model\\\\model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "C:\\bld\\opencv_1510966172919\\work\\opencv-3.3.0\\modules\\imgproc\\src\\color.cpp:10638: error: (-215) scn == 3 || scn == 4 in function cv::cvtColor\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-67038bba0992>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[0mimage1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage1Path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[0mimage_g\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthresh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_g\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTHRESH_BINARY\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTHRESH_OTSU\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: C:\\bld\\opencv_1510966172919\\work\\opencv-3.3.0\\modules\\imgproc\\src\\color.cpp:10638: error: (-215) scn == 3 || scn == 4 in function cv::cvtColor\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.externals import joblib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def extractHog(image):\n",
    "    image = cv2.resize(image, (32, 32))\n",
    "    winSize = (32, 32)  # Image Size\n",
    "    cellSize = (4, 4)  # Size of one cell\n",
    "    blockSizeInCells = (2, 2)  # will be multiplies by No of cells\n",
    "\n",
    "    blockSize = (blockSizeInCells[1] * cellSize[1], blockSizeInCells[0] * cellSize[0])\n",
    "    blockStride = (cellSize[1], cellSize[0])\n",
    "    nbins = 12  # Number of orientation bins\n",
    "    hog = cv2.HOGDescriptor(winSize, blockSize, blockStride, cellSize, nbins)  #\n",
    "    h = hog.compute(image)\n",
    "    h = h.flatten()\n",
    "    return h.flatten()\n",
    "\n",
    "\n",
    "def showImage(image):\n",
    "    b, g, r = cv2.split(image)  # get b,g,r\n",
    "    image = cv2.merge([r, g, b])  # switch it to rgb\n",
    "    fig, ax = plt.subplots(figsize=(4, 3))\n",
    "    ax.imshow(image, cmap=plt.cm.gray, interpolation='nearest')\n",
    "    ax.set_title('Image')\n",
    "    ax.axis('off')\n",
    "    ax.set_adjustable('box-forced')\n",
    "    plt.show()\n",
    "    cv2.waitKey(0)                 # Waits forever for user to press any key\n",
    "    cv2.destroyAllWindows()        # Closes displayed windows\n",
    "\n",
    "\n",
    "image1Path = 'C:\\\\Users\\\\Sherif\\\\Desktop\\\\IP_Project\\\\test8.jpg'\n",
    "image1 = cv2.imread(image1Path)\n",
    "\n",
    "image_g = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "ret, thresh = cv2.threshold(image_g, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "kernel = np.ones((3, 3), np.uint8)\n",
    "opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=2)\n",
    "it = 7\n",
    "sure_bg = cv2.dilate(opening, kernel, iterations=it)\n",
    "sure_fg = cv2.erode(sure_bg, kernel, iterations=it)\n",
    "\n",
    "unknown = cv2.subtract(sure_bg, sure_fg)\n",
    "\n",
    "ret2, markers = cv2.connectedComponents(sure_fg)\n",
    "markers = markers + 1\n",
    "markers[unknown == 255] = 0\n",
    "\n",
    "markers = cv2.watershed(image1, markers)\n",
    "image1[markers == -1] = [255, 0, 0]\n",
    "\n",
    "outputImages = list()\n",
    "avgSz = 0\n",
    "\n",
    "for region in range(ret2 - 1):\n",
    "    image_o = np.copy(image1)\n",
    "    image_o[markers != region + 2] = [0, 0, 0]\n",
    "    gray = cv2.cvtColor(image_o, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 1, 255, cv2.THRESH_BINARY)\n",
    "    im2, contours, hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnt = contours[0]\n",
    "    x, y, w, h = cv2.boundingRect(cnt)\n",
    "    image2 = image1[y:y + h, x:x + w]\n",
    "    if h > w:\n",
    "        image2 = np.rot90(image2, 1)\n",
    "    avgSz = avgSz + image2.size\n",
    "    outputImages.append(image2)\n",
    "\n",
    "avgSz = avgSz / (ret2 - 1)\n",
    "minSzThresh = 130000\n",
    "# maxSzThresh = 100000000\n",
    "\n",
    "for i in range(len(outputImages)):\n",
    "    if outputImages[i].size > minSzThresh:\n",
    "        image1 = outputImages[i]\n",
    "        image1Features = extractHog(image1)\n",
    "        testImages = list()\n",
    "        testImages.append(image1Features)\n",
    "\n",
    "#         NN_model = joblib.load('D:\\\\IP_Model\\\\model')\n",
    "        outputMat = NN_model.predict_proba(testImages)\n",
    "        ind = int(np.argmax(outputMat[0]) / 4)\n",
    "\n",
    "        if ind == 0:\n",
    "            print('\\nThe received bill was 5 pounds')\n",
    "        elif ind == 1:\n",
    "            print('\\nThe received bill was 10 pounds')\n",
    "        elif ind == 2:\n",
    "            print('\\nThe received bill was 20 pounds')\n",
    "        elif ind == 3:\n",
    "            print('\\nThe received bill was 50 pounds')\n",
    "        elif ind == 4:\n",
    "            print('\\nThe received bill was 100 pounds')\n",
    "        else:\n",
    "            print('\\nThe received bill was 200 pounds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
